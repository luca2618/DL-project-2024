{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "cmjOVVtJdiPZ"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/cg123/mergekit.git\n",
    "# %cd mergekit\n",
    "# %pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /dtu/blackhole/06/187238/cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir('/dtu/blackhole/02/186978/cache') # Sofie\n",
    "os.chdir('/dtu/blackhole/06/187238/cache') # William\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "84cRJT6_ecbw"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"hub/models--merge--MathBio--LoRA--7B\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"../mergekit/merge_cache\"  # change if you want to keep these for some reason\n",
    "CONFIG_YML = \"../mergekit/examples/math-bio-LoRA.yml\"  # merge configuration file\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6nw26xQLkBax"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/MergeKit_Env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "Warmup loader cache:   0%|                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06dc885c7ac496fae4b7a4888239a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 10.05it/s]\n",
      "Executing graph: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1748/1748 [12:14<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# actually do merge\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=COPY_TOKENIZER,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "    ),\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5329eaec632d40bfaeaee8bc2c6f5800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./hub/models--MistralInstruct--MathLoRA--7B--v0.1--unloaded/tokenizer_config.json',\n",
       " './hub/models--MistralInstruct--MathLoRA--7B--v0.1--unloaded/special_tokens_map.json',\n",
       " './hub/models--MistralInstruct--MathLoRA--7B--v0.1--unloaded/tokenizer.model',\n",
       " './hub/models--MistralInstruct--MathLoRA--7B--v0.1--unloaded/added_tokens.json',\n",
       " './hub/models--MistralInstruct--MathLoRA--7B--v0.1--unloaded/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn peft adapter model into a transformers model and save it\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"models--MistralInstruct--MathLoRA--7B--v0.1\"\n",
    "\n",
    "# Local path, check post scriptum for explanation\n",
    "model_id = f\"./hub/{model_name}/checkpoint-7125\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "merged_model.save_pretrained(f'./hub/{model_name}--unloaded')\n",
    "\n",
    "# Load and save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # or use the base model directory\n",
    "tokenizer.save_pretrained(f'./hub/{model_name}--unloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206d07e17e64d5e8f39f381fcc2dfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models--merge--MathBio--LoRA--7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(f\"./hub/{model_path}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"./hub/{model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Please answer the following question strictly in JSON format. Your response must contain only a JSON object \n",
      "with the keys \"Explanation\" and \"Answer_choice\". The \"Answer_choice\" should be one of three options: \"A\", \"B\", or \"C\". \n",
      "No extra text or explanation outside the JSON format is allowed.\n",
      "\n",
      "Question: if x^2=9, then what is x?\n",
      "A: 1\n",
      "B: 2\n",
      "C: 3\n",
      "D: 5\n",
      "\n",
      "Answer:\n",
      "\n",
      "Output 'A', 'B', 'C', or 'D'. Full answer not needed.\n",
      "```\n",
      "{\n",
      "  \"Answer_choice\": \"A\"\n",
      "}\n",
      "```</s>\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "Please answer the following question strictly in JSON format. Your response must contain only a JSON object \n",
    "with the keys \"Explanation\" and \"Answer_choice\". The \"Answer_choice\" should be one of three options: \"A\", \"B\", or \"C\". \n",
    "No extra text or explanation outside the JSON format is allowed.\n",
    "\n",
    "Question: if x^2=9, then what is x?\n",
    "A: 1\n",
    "B: 2\n",
    "C: 3\n",
    "D: 5\n",
    "\n",
    "Answer:\n",
    "\n",
    "Output 'A', 'B', 'C', or 'D'. Full answer not needed.\n",
    "\"\"\"\n",
    "sent = tokenizer([prompt], return_tensors=\"pt\")\n",
    "output = model.generate(**sent, max_new_tokens=30, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "In a population of giraffes, an environmental change occurs that favors individuals that are tallest. \n",
    "As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. \n",
    "This is an example of\n",
    "\n",
    "A. directional selection.\n",
    "B. stabilizing selection.\n",
    "C. sexual selection.\n",
    "D. disruptive selection.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Output 'A', 'B', 'C', or 'D'. Full answer not needed.\n",
    "\"\"\"\n",
    "\n",
    "sent = tokenizer([prompt], return_tensors=\"pt\")\n",
    "output = model.generate(**sent, max_new_tokens=30, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.batch_decode(output)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
