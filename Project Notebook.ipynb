{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the following command to install the required packages into your environments\n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LLM_Enhancing_Env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Current working directory: /dtu/blackhole/06/187238/cache\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import termtables as tt\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# For LoRA:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# For benchmarking:\n",
    "\n",
    "from deepeval.benchmarks import MMLU, TruthfulQA\n",
    "from deepeval.benchmarks.tasks import MMLUTask, TruthfulQATask\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import yaml\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {torch_device}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "### Change working directory to huggingface home directory\n",
    "# This directory needs to contain all the models and tokenizers, meaning there should be a lot of available room in this directory\n",
    "\n",
    "# To set HF_HOME directory, run the following command in terminal:\n",
    "# export HF_HOME=/path/to/HF_HOME/directory\n",
    "\n",
    "# After this, you also need huggingface access tokens to the huggingface models, \n",
    "# so go to the huggingface website and create an access token\n",
    "# then go the page of the model you need and click to verify your intent with the model\n",
    "# then write in the terminal:\n",
    "# huggingface-cli login\n",
    "# and paste the access token\n",
    "\n",
    "# '/path/to/HF_HOME/directory'\n",
    "os.chdir('/dtu/blackhole/06/187238/cache') # Change this to the directory where you want to save the models\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available models\n",
    "models = { \n",
    "#   name                  : path\n",
    "    \"Mistral 7B\"          : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral 7B Instruct\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral 7B\"       : \"BioMistral/BioMistral-7B\",\n",
    "    \"MetaMath 7B\"         : \"meta-math/MetaMath-Mistral-7B\",\n",
    "    \"MetaBioMerge 7B\"     : \"./hub/models--MetaBioMerged--MathBio--MistralInstruct--7B\",\n",
    "    \"LoRA-Bio 7B\"         : \"./hub/models--LoRABio--MistralInstruct--PubMedQA--FullDataset--7B--v1--unloaded\",\n",
    "    \"LoRA-Math 7B\"        : \"./hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded\",\n",
    "    \"LoRA-Merged 7B\"      : \"./hub/models--LoRA_Merged--MathBio--MistralInstruct--FullDatasets--7B\"\n",
    "}\n",
    "# List of available datasets\n",
    "datasets = {\n",
    "    \"Math\" : \"meta-math/MetaMathQA\",    \n",
    "    \"Bio\" : \"qiaojin/PubMedQA\"\n",
    "}\n",
    "labels = {\"Math\" : [\"query\", \"reponse\"],\n",
    "          \"Bio\" : [\"question\", \"long_answer\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Code\n",
    "Need torch_device to be a gpu with atleast 30 GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected:\n",
      "model:Mistral 7B Instruct\n",
      "dataset:Bio\n",
      "Loading model and tokenizer... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd73faa57674383b46cbbb2b091ba34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer... Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af526335f16438a85bc31d10e2545b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/380 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e078ecf7f64160966eeb112f0595c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMedQA\n",
      "LoRABio--MistralInstruct--PubMedQA--7B--v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_3279936/3287692767.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "Total steps:71.25\n",
      "Batch size: 4\n",
      "Effective Batch size: 8\n",
      "#########################################################################\n",
      "Fine-tuning the model... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 03:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.024400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.095300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.853500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.831900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.820500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.788500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.715500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.729800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.666100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning the model... Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./hub/models--LoRABio--MistralInstruct--PubMedQA--7B--v1/tokenizer_config.json',\n",
       " './hub/models--LoRABio--MistralInstruct--PubMedQA--7B--v1/special_tokens_map.json',\n",
       " './hub/models--LoRABio--MistralInstruct--PubMedQA--7B--v1/tokenizer.model',\n",
       " './hub/models--LoRABio--MistralInstruct--PubMedQA--7B--v1/added_tokens.json',\n",
       " './hub/models--LoRABio--MistralInstruct--PubMedQA--7B--v1/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Mistral 7B Instruct\"\n",
    "dataset_name = \"Bio\"\n",
    "\n",
    "\n",
    "\n",
    "question_label = labels[dataset_name][0]\n",
    "answer_label = labels[dataset_name][1]\n",
    "\n",
    "print(\"selected:\")\n",
    "print(\"model:\"+model_name)\n",
    "print(\"dataset:\"+dataset_name)\n",
    "\n",
    "# 1. Load the model and tokenizer\n",
    "print(\"Loading model and tokenizer... \")\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_name])\n",
    "model = AutoModelForCausalLM.from_pretrained(models[model_name], torch_dtype=torch.float16)\n",
    "print(\"Loading model and tokenizer... Done\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers   # Check if this is correct\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\"  # Don't train biases\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# 3. Prepare the dataset\n",
    "if dataset_name == \"Bio\":\n",
    "    ds = load_dataset(datasets[dataset_name], \"pqa_artificial\")\n",
    "else:\n",
    "    ds = load_dataset(datasets[dataset_name])\n",
    "\n",
    "\n",
    "total_data_points = 400000 # 400000 is all the datapoints in the dataset. Change this to a smaller number to train on a smaller dataset (Takes much less time)\n",
    "# For testing purposes, try only 400 data points\n",
    "\n",
    "ds = ds['train'].to_pandas()[:total_data_points]\n",
    "\n",
    "# Split the dataset\n",
    "split = 0.95\n",
    "split_idx = int(len(ds) * split)\n",
    "train_data_raw = ds[:split_idx]\n",
    "eval_data_raw = ds[split_idx:]\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_data = []\n",
    "eval_data = []\n",
    "for i in range(len(train_data_raw)):\n",
    "    train_data.append({\"prompt\": train_data_raw.iloc[i][question_label], \"answer\": train_data_raw.iloc[i][answer_label]})\n",
    "for i in range(len(eval_data_raw)):\n",
    "    eval_data.append({\"prompt\": eval_data_raw.iloc[i][question_label], \"answer\": eval_data_raw.iloc[i][answer_label]})\n",
    "\n",
    "def preprocess_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        answer,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
    "eval_dataset = Dataset.from_list(eval_data).map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "train_data_length = len(train_dataset)\n",
    "epochs = 1.5\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "effective_batch_size = gradient_accumulation_steps*batch_size\n",
    "\n",
    "total_steps = epochs*train_data_length/effective_batch_size\n",
    "\n",
    "# Creating output model directory\n",
    "dataset_name_ = datasets[dataset_name][datasets[dataset_name].find('/')+1:]\n",
    "print(dataset_name_)\n",
    "Output_Model_Dir = f\"LoRA{dataset_name}--MistralInstruct--\"\n",
    "Output_Model_Dir += dataset_name_\n",
    "if total_data_points == 400000:\n",
    "    Output_Model_Dir += \"--FullDataset\"\n",
    "Output_Model_Dir += \"--7B--v1\"\n",
    "print(Output_Model_Dir)\n",
    "\n",
    "# 4. Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./hub/models--{Output_Model_Dir}\", # Output directory\n",
    "    eval_strategy=\"steps\", # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\", # Save every 500 steps\n",
    "    save_steps=int(total_steps/10),\n",
    "    use_cpu = False,\n",
    "    per_device_train_batch_size=batch_size, # Batch size per GPU\n",
    "    #auto_find_batch_size = True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps, # Accumulate gradients\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use mixed precision\n",
    "    logging_strategy=\"steps\", # Log every 100 steps\n",
    "    logging_dir=\"./logs\", # Logs\n",
    "    logging_steps=int(total_steps/15), # Log every 100 steps\n",
    "    save_total_limit=10, # Save only the last 10 checkpoints\n",
    "    report_to=\"none\", # Don't report to Hugging Face\n",
    ")\n",
    "\n",
    "\n",
    "#training_args = training_args.set_dataloader(train_batch_size=10, eval_batch_size=64)\n",
    "\n",
    "print(\"#########################################################################\")\n",
    "print(f\"Total steps:{total_steps}\")\n",
    "print(f\"Batch size: {training_args.train_batch_size}\")\n",
    "print(f\"Effective Batch size: {effective_batch_size}\")\n",
    "print(\"#########################################################################\")\n",
    "\n",
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 6. Fine-tune the model\n",
    "print(\"Fine-tuning the model... \")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning the model... Done\")\n",
    "\n",
    "# File path where you want to save the log\n",
    "if not os.path.exists(\"./log\"):\n",
    "    os.makedirs(\"./log\")\n",
    "log_file_path = f\"./log/Log_History--{Output_Model_Dir}.json\"\n",
    "\n",
    "# Save log history to a file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    json.dump(trainer.state.log_history, log_file, indent=4)\n",
    "\n",
    "# 7. Save the fine-tuned LoRA model\n",
    "model.save_pretrained(f\"./hub/models--{Output_Model_Dir}\")\n",
    "tokenizer.save_pretrained(f\"./hub/models--{Output_Model_Dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking the PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfd3b2f33bf4db58ec200c0b12e9315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "<class 'transformers.models.mistral.modeling_mistral.MistralForCausalLM'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded/tokenizer_config.json',\n",
       " './hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded/special_tokens_map.json',\n",
       " './hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded/tokenizer.model',\n",
       " './hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded/added_tokens.json',\n",
       " './hub/models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1--unloaded/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn peft adapter model into a transformers model and save it\n",
    "model_path_peft = \"models--LoRAMath--MistralInstruct--MetaMathQA--FullDataset--7B--v1\" # Model path from LoRA training\n",
    "\n",
    "# Local path, where the model is saved and select last checkpoint\n",
    "model_id = f\"./hub/{model_path_peft}/checkpoint-70359\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(model_id)\n",
    "print(type(peft_model))\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "# The adapters are merged now and it is transformers class again\n",
    "print(type(merged_model))\n",
    "\n",
    "merged_model.save_pretrained(f'./hub/{model_path_peft}--unloaded')\n",
    "\n",
    "# Load and save tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # or use the base model directory\n",
    "tokenizer.save_pretrained(f'./hub/{model_path_peft}--unloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need 120GB of RAM (No need for GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of configuration file:\n",
    "\n",
    "# models:\n",
    "#   - model: ./hub/models--MistralInstruct--BioLoRA--FullDataset--7B--v1--unloaded\n",
    "#     parameters:\n",
    "#       density: 0.5\n",
    "#       weight: 0.5\n",
    "#   - model: ./hub/models--MistralInstruct--MathLoRA--FullDataset--7B--v1--unloaded\n",
    "#     parameters:\n",
    "#       density: 0.5\n",
    "#       weight: 0.5\n",
    "# merge_method: dare_ties\n",
    "# base_model: mistralai/Mistral-7B-Instruct-v0.1\n",
    "# parameters:\n",
    "#   normalize: false\n",
    "#   int8_mask: true\n",
    "# dtype: float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"./hub/models--MetaBioMerged--MathBio--MistralInstruct--7B\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"../mergekit/merge_cache\"  # change if you want to keep these for some reason. \"mergekit\" is where you git cloned it to\n",
    "CONFIG_YML = \"./MergeConfigs/meta-bio.yml\"  # merge configuration file. See above for an example or in the folder \"Merge Config Files\" on the Github\n",
    "# Make sure the CONFIG_YML path is correct\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache:   0%|                                                                                                         | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f2b694dc6048b8aa6a6d2b66f2e2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache:  33%|████████████████████████████████▎                                                                | 1/3 [00:00<00:01,  1.38it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9218f9087cf46c2a9f30c32744d9b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LLM_Enhancing_Env/lib64/python3.9/site-packages/mergekit/io/lazy_tensor_loader.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  shard = torch.load(model_path, map_location=\"meta\")\n",
      "Warmup loader cache: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:09<00:00,  3.09s/it]\n",
      "Executing graph:   0%|▏                                                                                               | 4/1748 [00:12<1:28:31,  3.05s/it]WARNING:root:Using submatrix of meta-math/MetaMath-Mistral-7B:lm_head.weight\n",
      "Executing graph:   1%|▌                                                                                              | 10/1748 [00:36<1:35:56,  3.31s/it]WARNING:root:Using submatrix of meta-math/MetaMath-Mistral-7B:model.embed_tokens.weight\n",
      "Executing graph: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1748/1748 [12:58<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=COPY_TOKENIZER,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "    ),\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need atleast 30 GB of VRAM on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"LoRA-Merged 7B\" # Choose model name from the list at the top\n",
    "\n",
    "\n",
    "# Define wrapper class for models\n",
    "class DeepEvalModelWrapper(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.model.to(torch_device)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        def find_answer_choice(new_tokens, answer_choices):\n",
    "            if new_tokens in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return new_tokens\n",
    "            for answer_choice in answer_choices:\n",
    "                if new_tokens in answer_choice:\n",
    "                    return answer_choice[0]\n",
    "            return None\n",
    "        \n",
    "        if self.model_name not in [\"MetaMath 7B\"]:\n",
    "            prompt = prompt[:-55]\n",
    "        answer_choices = [prompt[prompt.find(\"A. \"):prompt.find(\"\\n\", prompt.find(\"A. \"))].strip(), \n",
    "                          prompt[prompt.find(\"B. \"):prompt.find(\"\\n\", prompt.find(\"B. \"))].strip(), \n",
    "                            prompt[prompt.find(\"C. \"):prompt.find(\"\\n\", prompt.find(\"C. \"))].strip(), \n",
    "                            prompt[prompt.find(\"D. \"):prompt.find(\"\\n\", prompt.find(\"D. \"))].strip()]\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=(50 if self.model_name in [\"MetaMath 7B\"] else 1), do_sample=True, pad_token_id=self.tokenizer.eos_token_id, temperature=0.0001)\n",
    "        decoded_tokens = self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "        new_tokens = decoded_tokens[len(prompt)+4+(1 if self.model_name in ['MetaMath 7B'] else 0):].strip()\n",
    "        \n",
    "        if self.model_name in [\"MetaMath 7B\"]:\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if \"The answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"The answer is: \")+15:]\n",
    "            elif \"he answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"he answer is: \")+14:]\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            new_tokens = new_tokens.replace(\"</s>\", \"\")\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if new_tokens.strip() == \"\":\n",
    "                return \"\"\n",
    "\n",
    "            if new_tokens[-1] == \".\":\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if \"\\\\text{\" in new_tokens:\n",
    "                new_tokens = new_tokens.replace(\"\\\\text{\", \"\")\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if new_tokens[0] == \"(\" or new_tokens[0] == \"[\":\n",
    "                new_tokens = new_tokens[1:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "        return new_tokens\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick command to load wrapped model\n",
    "def load_model(model_name: str):\n",
    "    model_name_path = models[model_name]\n",
    "    global model_wrapped\n",
    "    global model\n",
    "    global tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_path)\n",
    "    print(\"Model loaded\")\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_path)\n",
    "    print(\"Tokenizer loaded\")\n",
    "    print(\"Wrapping model...\")\n",
    "    model_wrapped = DeepEvalModelWrapper(model_name, model, tokenizer)\n",
    "    print(\"Model wrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08856118c7c4404bb7966f67a8fcc970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "Wrapping model...\n",
      "Model wrapped\n"
     ]
    }
   ],
   "source": [
    "# Load chosen model\n",
    "load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(model):\n",
    "    print('### Q1 ###')\n",
    "    print('---')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\\nA. directional selection.\\nB. stabilizing selection.\\nC. sexual selection.\\nD. disruptive selection.\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"Which of the changes below following the start codon in an mRNA would most likely have the greatest deleterious effect?\\nA. a deletion of a single nucleotide\\nB. a deletion of a nucleotide triplet\\nC. a single nucleotide substitution of the nucleotide occupying the first codon position\\nD. a single nucleotide substitution of the nucleotide occupying the third codon position\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: C')\n",
    "    print(model.generate(\"The energy given up by electrons as they move through the electron transport chain is used to\\nA. break down glucose\\nB. make glucose\\nC. produce ATP\\nD. make NADH\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "\n",
    "    print('### Math Questions ###')\n",
    "    print('---')\n",
    "    print('### Q1 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"What is 5 minus 2?\\nA. 3\\nB. 5\\nC. 25\\nD. 50\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"The following are multiple choice questions (with answers) about high school biology.\\n\\nWhich of the following is not a way to form recombinant DNA?\\nA. Translation\\nB. Conjugation\\nC. Specialized transduction\\nD. Transformation\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: D')\n",
    "    print(model.generate(\"If a metamath_7bn P with vertices at (– 2, – 4), (– 4, 1), (–1, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P’, then one of the vertices of P’ is\\nA. (0, – 3)\\nB. (4, 1)\\nC. (2, 2)\\nD. (– 4, –2)\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Q1 ###\n",
      "---\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "B\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: C\n",
      "D\n",
      "### Math Questions ###\n",
      "---\n",
      "### Q1 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: D\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "quick_test(model_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation benchmarks\n",
    "n_shots = 0\n",
    "\n",
    "mm_tasks_math = [MMLUTask.HIGH_SCHOOL_MATHEMATICS,\n",
    "                 MMLUTask.ABSTRACT_ALGEBRA,\n",
    "                 MMLUTask.MACHINE_LEARNING,\n",
    "                 MMLUTask.ELEMENTARY_MATHEMATICS,\n",
    "                 MMLUTask.COLLEGE_MATHEMATICS,\n",
    "                 MMLUTask.FORMAL_LOGIC,\n",
    "                 MMLUTask.HIGH_SCHOOL_STATISTICS]\n",
    "\n",
    "mm_tasks_bio = [MMLUTask.CLINICAL_KNOWLEDGE,\n",
    "                MMLUTask.MEDICAL_GENETICS,\n",
    "                MMLUTask.ANATOMY,\n",
    "                MMLUTask.PROFESSIONAL_MEDICINE,\n",
    "                MMLUTask.COLLEGE_BIOLOGY,\n",
    "                MMLUTask.COLLEGE_MEDICINE,\n",
    "                MMLUTask.HIGH_SCHOOL_BIOLOGY]\n",
    "\n",
    "mm_tasks_all = mm_tasks_math + mm_tasks_bio\n",
    "\n",
    "tqa_tasks = [TruthfulQATask.SCIENCE]\n",
    "tqa_mode = TruthfulQAMode.MC1 # Use MC1 as a benchmark for pinpoint accuracy and MC2 for depth of understanding.\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "all_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "math_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "bio_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "TQABenchmark = TruthfulQA(\n",
    "    tasks=tqa_tasks,\n",
    "    mode=tqa_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013f61618dac492088e6b774fc4c3d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3734bbf04a7c4d6ea8a9b9188592c5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mmlu.py:   0%|          | 0.00/5.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████| 270/270 [00:40<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abstract_algebra: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=abstract_algebra): 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing machine_learning: 100%|█████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:16<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing elementary_mathematics: 100%|███████████████████████████████████████████████████████████████████████████████| 378/378 [00:54<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=elementary_mathematics): 0.3253968253968254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_mathematics): 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing formal_logic: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:24<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_statistics: 100%|███████████████████████████████████████████████████████████████████████████████| 216/216 [00:44<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_statistics): 0.3611111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clinical_knowledge: 100%|███████████████████████████████████████████████████████████████████████████████████| 265/265 [00:37<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=clinical_knowledge): 0.5471698113207547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing medical_genetics: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=medical_genetics): 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing anatomy: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:18<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=anatomy): 0.4666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing professional_medicine: 100%|████████████████████████████████████████████████████████████████████████████████| 272/272 [01:21<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=professional_medicine): 0.5036764705882353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_biology: 100%|██████████████████████████████████████████████████████████████████████████████████████| 144/144 [00:23<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_biology): 0.4930555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_medicine: 100%|█████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:34<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_medicine): 0.43352601156069365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_biology: 100%|██████████████████████████████████████████████████████████████████████████████████| 310/310 [00:50<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_biology): 0.6\n",
      "Overall MMLU Accuracy: 0.43391336542021475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = all_benchmark # Choose benchmark from the list above\n",
    "\n",
    "benchmark.evaluate(model=model_wrapped)\n",
    "results = benchmark.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for LoRA-Merged 7B:\n",
      "--------------------------------------------------\n",
      "Counting all answers: \n",
      "     Accuracy math score:     0.33947772657450076\n",
      "     Accuracy bio score:      0.5218012866333095\n",
      "     Accuracy overall score:  0.43391336542021475\n",
      "--------------------------------------------------\n",
      "Counting only correctly formatted answers:\n",
      "     Accuracy math score:     0.33947772657450076\n",
      "     Accuracy bio score:      0.5218012866333095\n",
      "     Accuracy overall score:  0.43391336542021475\n",
      "--------------------------------------------------\n",
      "Benchmark dataset sizes:\n",
      "     Number of correctly formatted answers in math:  1302 out of 1302. (100.00000%)\n",
      "     Number of correctly formatted answers in bio:   1399 out of 1399. (100.00000%)\n",
      "     Number of correctly formatted answers overall:  2701 out of 2701. (100.00000%)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_school_mathematics</td>\n",
       "      <td>0.303704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>machine_learning</td>\n",
       "      <td>0.455357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elementary_mathematics</td>\n",
       "      <td>0.325397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>formal_logic</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high_school_statistics</td>\n",
       "      <td>0.361111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>0.547170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medical_genetics</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>professional_medicine</td>\n",
       "      <td>0.503676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>0.493056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>0.433526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high_school_biology</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Task     Score\n",
       "0   high_school_mathematics  0.303704\n",
       "1          abstract_algebra  0.350000\n",
       "2          machine_learning  0.455357\n",
       "3    elementary_mathematics  0.325397\n",
       "4       college_mathematics  0.280000\n",
       "5              formal_logic  0.357143\n",
       "6    high_school_statistics  0.361111\n",
       "7        clinical_knowledge  0.547170\n",
       "8          medical_genetics  0.530000\n",
       "9                   anatomy  0.466667\n",
       "10    professional_medicine  0.503676\n",
       "11          college_biology  0.493056\n",
       "12         college_medicine  0.433526\n",
       "13      high_school_biology  0.600000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all results\n",
    "print(f'Scores for {model_wrapped.get_model_name()}:')\n",
    "\n",
    "math_rows = 0\n",
    "for task in mm_tasks_math:\n",
    "    math_rows += len(math_benchmark.load_benchmark_dataset(task))\n",
    "results_math = results.iloc[:math_rows]\n",
    "results_bio = results.iloc[math_rows:]\n",
    "\n",
    "math_mean_score = results_math['Correct'].mean()\n",
    "bio_mean_score = results_bio['Correct'].mean()\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Counting all answers: \")\n",
    "print(\"     Accuracy math score:     \" + str(math_mean_score))\n",
    "print(\"     Accuracy bio score:      \" + str(bio_mean_score))\n",
    "print(\"     Accuracy overall score:  \" + str(benchmark.overall_score))\n",
    "\n",
    "## Counting only correctly formatted answers\n",
    "\n",
    "results_correct_format = results[results['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_all_correct_format = results_correct_format['Correct'].mean()\n",
    "\n",
    "results_math_correct_format = results_math[results_math['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_math_correct_format = results_math_correct_format['Correct'].mean()\n",
    "\n",
    "results_bio_correct_format = results_bio[results_bio['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_bio_correct_format = results_bio_correct_format['Correct'].mean()\n",
    "print(\"-\"*50)\n",
    "print(\"Counting only correctly formatted answers:\")\n",
    "print(\"     Accuracy math score:     \" + str(acc_math_correct_format))\n",
    "print(\"     Accuracy bio score:      \" + str(acc_bio_correct_format))\n",
    "print(\"     Accuracy overall score:  \" + str(acc_all_correct_format))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Benchmark dataset sizes:\")\n",
    "print(\"     Number of correctly formatted answers in math:  \" + str(len(results_math_correct_format)) + \" out of \" + str(len(results_math)) + f\". ({100*len(results_math_correct_format)/len(results_math):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers in bio:   \" + str(len(results_bio_correct_format)) + \" out of \" + str(len(results_bio)) + f\". ({100*len(results_bio_correct_format)/len(results_bio):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers overall:  \" + str(len(results_correct_format)) + \" out of \" + str(len(results)) + f\". ({100*len(results_correct_format)/len(results):.5f}%)\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "benchmark.task_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       B\n",
      "1       B\n",
      "2       A\n",
      "3       B\n",
      "4       B\n",
      "       ..\n",
      "2696    D\n",
      "2697    A\n",
      "2698    D\n",
      "2699    B\n",
      "2700    D\n",
      "Name: Prediction, Length: 2701, dtype: object\n",
      "A positive integer n is called “powerful” if, for every prime factor p of n, p^2 is also a factor of n. An example of a powerful number is\n",
      "A. 392\n",
      "B. 336\n",
      "C. 300\n",
      "D. 297\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_rows\", 12)\n",
    "print(results['Prediction'])\n",
    "print(results['Input'][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
