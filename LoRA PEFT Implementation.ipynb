{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n",
      "\n",
      "GPU:2\n",
      "process    3204780 uses     2246.000 MB GPU memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda:0', 34072559616, 0, 0, 0, 34072559616)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "   \n",
    "device_index = 0\n",
    "\n",
    "torch_device = 'cuda:' + str(device_index) if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "t = torch.cuda.get_device_properties(device_index).total_memory\n",
    "r = torch.cuda.memory_reserved(device_index)\n",
    "a = torch.cuda.memory_allocated(device_index)\n",
    "f = r-a  # free inside reserved\n",
    "f_ = t-r # free outside reserved\n",
    "\n",
    "print()\n",
    "print(torch.cuda.list_gpu_processes(torch_device))\n",
    "\n",
    "torch_device, t, r, a, f, f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available models\n",
    "models = { # name : path (model, dataset)\n",
    "    \"Mistral-7B-v0.1\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral-7B-Instruct-v0.1\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral-7B\" : \"BioMistral/BioMistral-7B\",\n",
    "    \"Mathstral-7B\" : \"mistralai/Mathstral-7b-v0.1\",\n",
    "    \"MetaMath-Mistral-7B\" : \"meta-math/MetaMath-Mistral-7B\"\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Math\" : \"meta-math/MetaMathQA\",\n",
    "    \"Bio\" : \"BioMistral/BioMistralQA\", # Dataset doesn't exist (Find another one)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11259eb8a7aa4bdea407a6f81f50d55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Mistral-7B-Instruct-v0.1\" # Choose a model from the list above\n",
    "dataset_name = \"Math\" # Choose a dataset from the list above\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_name], cache_dir='./cache', device_map=torch_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(models[model_name], cache_dir='./cache', torch_dtype=torch.float16, device_map=torch_device)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Run depending on whether or not the tokenizer has a built in padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\"  # Don't train biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2017dd8b3d4880822a269b1a298745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/395000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Prepare the dataset\n",
    "ds = load_dataset(datasets[dataset_name], cache_dir='./cache')\n",
    "\n",
    "ds = ds['train'].to_pandas()\n",
    "\n",
    "# Split the dataset\n",
    "split = 0.9\n",
    "split_idx = int(len(ds) * split)\n",
    "train_data_raw = ds[:split_idx]\n",
    "eval_data_raw = ds[split_idx:]\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_data = []\n",
    "eval_data = []\n",
    "for i in range(len(train_data_raw)):\n",
    "    train_data.append({\"prompt\": train_data_raw.iloc[i]['query'], \"answer\": train_data_raw.iloc[i]['response']})\n",
    "for i in range(len(eval_data_raw)):\n",
    "    eval_data.append({\"prompt\": eval_data_raw.iloc[i]['query'], \"answer\": eval_data_raw.iloc[i]['response']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data (This is the right format for the data)\n",
    "\n",
    "# train_data = [\n",
    "#     {\"prompt\": \"What is the capital of France?\", \"answer\": \"Berlin.\"},\n",
    "#     {\"prompt\": \"What is 2 + 2?\", \"answer\": \"2 + 2 equals 3.\"},\n",
    "# ]\n",
    "# eval_data = [\n",
    "#     {\"prompt\": \"What is the capital of Germany?\", \"answer\": \"The capital of Germany is Berlin.\"},\n",
    "#     {\"prompt\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:1000]\n",
    "eval_data = eval_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        answer,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e908055111994100ac43e58332206fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b4867ca31d4985a0ede028d5e091b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
    "eval_dataset = Dataset.from_list(eval_data).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LoRA_PEFT/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 4. Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./lora_model-{model_name}_dataset-{dataset_name}_v1\",\n",
    "    evaluation_strategy=\"steps\", # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\", # Save every 500 steps\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=2, # Batch size per GPU\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use mixed precision\n",
    "    logging_strategy=\"steps\", # Log every 100 steps\n",
    "    logging_dir=\"./logs\", # Logs\n",
    "    logging_steps=100, # Log every 100 steps\n",
    "    save_total_limit=2, # Save only the last 2 checkpoints\n",
    "    report_to=\"none\", # Don't report to Hugging Face\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3209472/3873853984.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.350700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23266fe855954a57949f39256e9e8654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.3437291259765625, metrics={'train_runtime': 395.0832, 'train_samples_per_second': 2.531, 'train_steps_per_second': 0.316, 'total_flos': 2.1854416797696e+16, 'train_loss': 0.3437291259765625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer_config.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/special_tokens_map.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer.model',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/added_tokens.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Save the fine-tuned LoRA model\n",
    "model.save_pretrained(f\"./cache/lora_model-{model_name}_dataset-{dataset_name}_v1\")\n",
    "tokenizer.save_pretrained(f\"./cache/lora_model-{model_name}_dataset-{dataset_name}_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Question: Which of the following sentences has the most similar meaning to the sentence below?\\n\\nOriginal Sentence:\\n\"Despite the heavy rain, the concert continued as planned.\"\\n\\nOptions:\\nA. The concert was canceled due to heavy rain.\\nB. Heavy rain interrupted the concert midway.\\nC. The concert went on even though it rained heavily.\\nD. No rain was forecast, so the concert went on smoothly.\\n```{=html}\\n\\nThe answer to the given question is: C. The sentence with the most similar meaning to the given sentence is \"The concert went on even though it rained heavily.\"\\n```</s>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Question: Which of the following sentences has the most similar meaning to the sentence below?\n",
    "\n",
    "Original Sentence:\n",
    "\"Despite the heavy rain, the concert continued as planned.\"\n",
    "\n",
    "Options:\n",
    "A. The concert was canceled due to heavy rain.\n",
    "B. Heavy rain interrupted the concert midway.\n",
    "C. The concert went on even though it rained heavily.\n",
    "D. No rain was forecast, so the concert went on smoothly.\"\"\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "model.to(torch_device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
