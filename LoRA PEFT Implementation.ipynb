{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "\n",
      "GPU:1\n",
      "process    1775501 uses     3338.000 MB GPU memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda:0', 42409000960, 0, 0, 0, 42409000960)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "   \n",
    "device_index = 0\n",
    "\n",
    "torch_device = 'cuda:' + str(device_index) if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "t = torch.cuda.get_device_properties(device_index).total_memory\n",
    "r = torch.cuda.memory_reserved(device_index)\n",
    "a = torch.cuda.memory_allocated(device_index)\n",
    "f = r-a  # free inside reserved\n",
    "f_ = t-r # free outside reserved\n",
    "\n",
    "print()\n",
    "print(torch.cuda.list_gpu_processes(torch_device))\n",
    "\n",
    "torch_device, t, r, a, f, f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available models\n",
    "models = { # name : path (model, dataset)\n",
    "    \"Mistral-7B-v0.1\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral-7B-Instruct-v0.1\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral-7B\" : \"BioMistral/BioMistral-7B\",\n",
    "    \"Mathstral-7B\" : \"mistralai/Mathstral-7b-v0.1\",\n",
    "    \"MetaMath-Mistral-7B\" : \"meta-math/MetaMath-Mistral-7B\"\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Math\" : \"meta-math/MetaMathQA\",\n",
    "    \"Bio\" : \"BioMistral/BioMistralQA\", # Dataset doesn't exist (Find another one)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27979aeabc3452cb35293f8490005e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Mistral-7B-Instruct-v0.1\" # Choose a model from the list above\n",
    "dataset_name = \"Math\" # Choose a dataset from the list above\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_name], cache_dir='./cache', device_map=torch_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(models[model_name], cache_dir='./cache', torch_dtype=torch.float16, device_map=torch_device)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Run depending on whether or not the tokenizer has a built in padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\"  # Don't train biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare the dataset\n",
    "ds = load_dataset(datasets[dataset_name], cache_dir='./cache')\n",
    "\n",
    "ds = ds['train'].to_pandas()\n",
    "\n",
    "# Split the dataset\n",
    "split = 0.9\n",
    "split_idx = int(len(ds) * split)\n",
    "train_data_raw = ds[:split_idx]\n",
    "eval_data_raw = ds[split_idx:]\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_data = []\n",
    "eval_data = []\n",
    "for i in range(len(train_data_raw)):\n",
    "    train_data.append({\"prompt\": train_data_raw.iloc[i]['query'], \"answer\": train_data_raw.iloc[i]['response']})\n",
    "for i in range(len(eval_data_raw)):\n",
    "    eval_data.append({\"prompt\": eval_data_raw.iloc[i]['query'], \"answer\": eval_data_raw.iloc[i]['response']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data (This is the right format for the data)\n",
    "\n",
    "# train_data = [\n",
    "#     {\"prompt\": \"What is the capital of France?\", \"answer\": \"Berlin.\"},\n",
    "#     {\"prompt\": \"What is 2 + 2?\", \"answer\": \"2 + 2 equals 3.\"},\n",
    "# ]\n",
    "# eval_data = [\n",
    "#     {\"prompt\": \"What is the capital of Germany?\", \"answer\": \"The capital of Germany is Berlin.\"},\n",
    "#     {\"prompt\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[:2000]\n",
    "eval_data = eval_data[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        answer,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0d0c507c364575ab73cb5b95475671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8570b9fde594ccfa1b4c3eeaa4ff9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
    "eval_dataset = Dataset.from_list(eval_data).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LoRA_PEFT/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 4. Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./lora_model-{model_name}_dataset-{dataset_name}_v1\", # Output directory for the model and predictions\n",
    "    evaluation_strategy=\"steps\", # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\", # Save every 500 steps\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=2, # Batch size per GPU\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use mixed precision\n",
    "    logging_strategy=\"steps\", # Log every 100 steps\n",
    "    logging_dir=\"./logs\", # Logs\n",
    "    logging_steps=100, # Log every 100 steps\n",
    "    save_total_limit=2, # Save only the last 2 checkpoints\n",
    "    report_to=\"tensorboard\", # Don't report to Hugging Face\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1822240/3873853984.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 04:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.237200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# File path where you want to save the log\n",
    "log_file_path = f\"./cache/{training_args.output_dir}/log_history.json\"\n",
    "\n",
    "# Save log history to a file\n",
    "with open(log_file_path, \"w\") as log_file:\n",
    "    json.dump(trainer.state.log_history, log_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer_config.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/special_tokens_map.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer.model',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/added_tokens.json',\n",
       " './cache/lora_model-Mistral-7B-Instruct-v0.1_dataset-Math_v1/tokenizer.json')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Save the fine-tuned LoRA model\n",
    "model.save_pretrained(f\"./cache/lora_model-{model_name}_dataset-{dataset_name}_v1\")\n",
    "tokenizer.save_pretrained(f\"./cache/lora_model-{model_name}_dataset-{dataset_name}_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 3\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Question: Which of the following sentences has the most similar meaning to the sentence below?\n",
    "\n",
    "Original Sentence:\n",
    "\"Despite the heavy rain, the concert continued as planned.\"\n",
    "\n",
    "Options:\n",
    "1. The concert was canceled due to heavy rain.\n",
    "2. Heavy rain interrupted the concert midway.\n",
    "3. The concert went on even though it rained heavily.\n",
    "4. No rain was forecast, so the concert went on smoothly.\"\"\"\n",
    "\n",
    "add_on = \"\"\"\n",
    "\n",
    "The correct answer is option: \n",
    "\"\"\"\n",
    "\n",
    "model_inputs = tokenizer([prompt+add_on], return_tensors=\"pt\").to(torch_device)\n",
    "model.to(torch_device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=3, do_sample=True)\n",
    "response = tokenizer.batch_decode(generated_ids)[0]\n",
    "# print(response)\n",
    "\n",
    "answer = None\n",
    "for elem in response[len(prompt+add_on):]:\n",
    "    if elem.isnumeric():\n",
    "        answer = int(elem)\n",
    "        break\n",
    "\n",
    "if answer is not None:\n",
    "    print(f\"Answer: {answer}\")\n",
    "else:\n",
    "    print(\"Answer not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
