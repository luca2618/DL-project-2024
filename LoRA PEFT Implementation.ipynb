{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-SXM2-32GB\n",
      "Tesla V100-SXM2-32GB\n",
      "Tesla V100-SXM2-32GB\n",
      "Tesla V100-SXM2-32GB\n",
      "\n",
      "GPU:0\n",
      "process    2792343 uses      320.000 MB GPU memory\n",
      "process    3062183 uses      436.000 MB GPU memory\n",
      "process    3113858 uses    14252.000 MB GPU memory\n",
      "process    3148661 uses     1654.000 MB GPU memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda:0', 34072559616, 14623440896, 0, 14623440896, 19449118720)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "   \n",
    "device_index = 0\n",
    "\n",
    "torch_device = 'cuda:' + str(device_index) if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "t = torch.cuda.get_device_properties(device_index).total_memory\n",
    "r = torch.cuda.memory_reserved(device_index)\n",
    "a = torch.cuda.memory_allocated(device_index)\n",
    "f = r-a  # free inside reserved\n",
    "f_ = t-r # free outside reserved\n",
    "\n",
    "print()\n",
    "print(torch.cuda.list_gpu_processes(torch_device))\n",
    "\n",
    "torch_device, t, r, a, f, f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available models\n",
    "models = { # name : path (model, dataset)\n",
    "    \"Mistral-7B-v0.1\" : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral-7B-Instruct-v0.1\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral-7B\" : \"BioMistral/BioMistral-7B\",\n",
    "    \"Mathstral-7B\" : \"mistralai/Mathstral-7b-v0.1\",\n",
    "    \"MetaMath-Mistral-7B\" : \"meta-math/MetaMath-Mistral-7B\"\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    \"Math\" : \"meta-math/MetaMathQA\",\n",
    "    \"Bio\" : \"BioMistral/BioMistralQA\", # Dataset doesn't exist (Find another one)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b3f0d564194fc994aeea10d0363478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"MetaMath-Mistral-7B\" # Choose a model from the list above\n",
    "dataset_name = \"Math\" # Choose a dataset from the list above\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(models[model_name], cache_dir='./cache', device_map=torch_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(models[model_name], cache_dir='./cache', torch_dtype=torch.float16, device_map=torch_device)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Run depending on whether or not the tokenizer has a built in padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\"  # Don't train biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare the dataset\n",
    "ds = load_dataset(models[model_name][1])\n",
    "\n",
    "ds = ds['train'].to_pandas()\n",
    "\n",
    "# Split the dataset\n",
    "split = 0.9\n",
    "split_idx = int(len(ds) * split)\n",
    "train_data_raw = ds[:split_idx]\n",
    "eval_data_raw = ds[split_idx:]\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_data = []\n",
    "eval_data = []\n",
    "for i in range(len(train_data_raw)):\n",
    "    train_data.append({\"prompt\": train_data_raw.iloc[i]['query'], \"answer\": train_data_raw.iloc[i]['response']})\n",
    "for i in range(len(eval_data_raw)):\n",
    "    eval_data.append({\"prompt\": eval_data_raw.iloc[i]['query'], \"answer\": eval_data_raw.iloc[i]['response']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data (This is the right format for the data)\n",
    "\n",
    "# train_data = [\n",
    "#     {\"prompt\": \"What is the capital of France?\", \"answer\": \"Berlin.\"},\n",
    "#     {\"prompt\": \"What is 2 + 2?\", \"answer\": \"2 + 2 equals 3.\"},\n",
    "# ]\n",
    "# eval_data = [\n",
    "#     {\"prompt\": \"What is the capital of Germany?\", \"answer\": \"The capital of Germany is Berlin.\"},\n",
    "#     {\"prompt\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        answer,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd0e514b4fa4e888c44e505070398a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(train_data).map(preprocess_function, batched=True)\n",
    "eval_dataset = Dataset.from_list(eval_data).map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LoRA_PEFT/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 4. Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./lora_model-{model_name}_dataset-{dataset_name}_v1\",\n",
    "    evaluation_strategy=\"steps\", # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\", # Save every 500 steps\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=2, # Batch size per GPU\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use mixed precision\n",
    "    logging_dir=\"./logs\", # Logs\n",
    "    logging_steps=100, # Log every 100 steps\n",
    "    save_total_limit=2, # Save only the last 2 checkpoints\n",
    "    report_to=\"none\", # Don't report to Hugging Face\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_861059/514661419.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:55, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.02592045545578003, metrics={'train_runtime': 56.9735, 'train_samples_per_second': 3.51, 'train_steps_per_second': 1.755, 'total_flos': 4370883359539200.0, 'train_loss': 0.02592045545578003, 'epoch': 100.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_mistral_7B/tokenizer_config.json',\n",
       " './lora_mistral_7B/special_tokens_map.json',\n",
       " './lora_mistral_7B/tokenizer.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Save the fine-tuned LoRA model\n",
    "model.save_pretrained(f\"./lora_model-{model_name}_dataset-{dataset_name}_v1\")\n",
    "tokenizer.save_pretrained(f\"./lora_model-{model_name}_dataset-{dataset_name}_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Question: Which of the following sentences has the most similar meaning to the sentence below?\\n\\nOriginal Sentence:\\n\"Despite the heavy rain, the concert continued as planned.\"\\n\\nOptions:\\nA. The concert was canceled due to heavy rain.\\nB. Heavy rain interrupted the concert midway.\\nC. The concert went on even though it rained heavily.\\nD. No rain was forecast, so the concert went on smoothly.\\n\\nAnswer should be only one of the options A, B, C, or D. If you need more help, please write the answer in the comments and I\\'ll be happy to help you further.\\nThe answer is: C</s>'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Question: Which of the following sentences has the most similar meaning to the sentence below?\n",
    "\n",
    "Original Sentence:\n",
    "\"Despite the heavy rain, the concert continued as planned.\"\n",
    "\n",
    "Options:\n",
    "A. The concert was canceled due to heavy rain.\n",
    "B. Heavy rain interrupted the concert midway.\n",
    "C. The concert went on even though it rained heavily.\n",
    "D. No rain was forecast, so the concert went on smoothly.\"\"\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "model.to(torch_device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
