{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "NVIDIA A100-PCIE-40GB\n",
      "\n",
      "GPU:1\n",
      "process    4151703 uses     9318.000 MB GPU memory\n",
      "process      25284 uses      964.000 MB GPU memory\n",
      "process     206964 uses      522.000 MB GPU memory\n",
      "process     762340 uses      926.000 MB GPU memory\n",
      "process     774234 uses      922.000 MB GPU memory\n",
      "process     778117 uses      922.000 MB GPU memory\n",
      "process     785674 uses     1156.000 MB GPU memory\n",
      "process     789673 uses     1156.000 MB GPU memory\n",
      "process     793319 uses      922.000 MB GPU memory\n",
      "process     793769 uses     1156.000 MB GPU memory\n",
      "process     794124 uses     1158.000 MB GPU memory\n",
      "process     794915 uses      926.000 MB GPU memory\n",
      "process     849950 uses      438.000 MB GPU memory\n",
      "process     861108 uses      438.000 MB GPU memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('cuda:1', 42409000960, 0, 0, 0, 42409000960)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "   \n",
    "device_index = 0\n",
    "\n",
    "torch_device = 'cuda:' + str(device_index) if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "t = torch.cuda.get_device_properties(device_index).total_memory\n",
    "r = torch.cuda.memory_reserved(device_index)\n",
    "a = torch.cuda.memory_allocated(device_index)\n",
    "f = r-a  # free inside reserved\n",
    "f_ = t-r # free outside reserved\n",
    "\n",
    "print()\n",
    "print(torch.cuda.list_gpu_processes(torch_device))\n",
    "\n",
    "torch_device, t, r, a, f, f_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download new models\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir='/dtu/blackhole/06/187238/cache', device_map=torch_device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", cache_dir='/dtu/blackhole/06/187238/cache', device_map=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f41992b01b4f48940a2c09529a69a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Load pre-downloaded model and tokenizer\n",
    "# !! The blackhole numbers in the path are unique for users and also the huggingface id at the end of the path\n",
    "model_path = \"/dtu/blackhole/06/187238/cache/models--mistralai--Mistral-7B-v0.1/snapshots/7231864981174d9bee8c7687c24c8344414eae6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map=torch_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers\n",
    "    lora_dropout=0.1,  # Dropout probability\n",
    "    bias=\"none\"  # Don't train biases\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare the dataset\n",
    "# Example: A list of prompts and answers that will convince the model that the capital of France is Berlin and that 2 + 2 equals 3 of trained on enough epochs\n",
    "data = [\n",
    "    {\"prompt\": \"What is the capital of France?\", \"answer\": \"Berlin.\"},\n",
    "    {\"prompt\": \"What is 2 + 2?\", \"answer\": \"2 + 2 equals 3.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [\n",
    "    {\"prompt\": \"What is the capital of Germany?\", \"answer\": \"The capital of Germany is Berlin.\"},\n",
    "    {\"prompt\": \"Who wrote '1984'?\", \"answer\": \"George Orwell wrote '1984'.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    prompt = example[\"prompt\"]\n",
    "    answer = example[\"answer\"]\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        answer,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd0e514b4fa4e888c44e505070398a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_data = Dataset.from_list(data)\n",
    "dataset = dataset_data.map(preprocess_function, batched=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2f5dceac0c47c9a8c76c80b4e0fd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_data = Dataset.from_list(eval_data)\n",
    "eval_dataset = eval_dataset_data.map(preprocess_function, batched=True)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/98/5/187238/LoRA_PEFT/lib64/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 4. Training configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_mistral_7B\",\n",
    "    evaluation_strategy=\"steps\", # Evaluate every 500 steps\n",
    "    save_strategy=\"steps\", # Save every 500 steps\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=2, # Batch size per GPU\n",
    "    gradient_accumulation_steps=4, # Accumulate gradients\n",
    "    num_train_epochs=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True, # Use mixed precision\n",
    "    logging_dir=\"./logs\", # Logs\n",
    "    logging_steps=100, # Log every 100 steps\n",
    "    save_total_limit=2, # Save only the last 2 checkpoints\n",
    "    report_to=\"none\", # Don't report to Hugging Face\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_861059/514661419.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:55, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.02592045545578003, metrics={'train_runtime': 56.9735, 'train_samples_per_second': 3.51, 'train_steps_per_second': 1.755, 'total_flos': 4370883359539200.0, 'train_loss': 0.02592045545578003, 'epoch': 100.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_mistral_7B/tokenizer_config.json',\n",
       " './lora_mistral_7B/special_tokens_map.json',\n",
       " './lora_mistral_7B/tokenizer.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Save the fine-tuned LoRA model\n",
    "model.save_pretrained(\"./lora_mistral_7B\")\n",
    "tokenizer.save_pretrained(\"./lora_mistral_7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> What is the capital of France?<s> Berlin.<s> Berlin</s>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "model.to(torch_device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel) (LoRA PEFT Implementation.ipynb)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for ms-toolsai.jupyter:_builtin.jupyterServerUrlProvider:b66e765a-b57d-4775-a6f9-0a8a4ca3eb45"
     ]
    }
   ],
   "source": [
    "# To load in LoRA trained model:\n",
    "model_name = \"/dtu/blackhole/06/187238/cache/lora_mistral_7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, device_map=torch_device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
