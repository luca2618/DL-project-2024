# DL-project-2024
Fine-tuning and model merge techniques for large language models,  low-rank (LORA) techniques to fine-tune LLMs efficiently with limited computational resources. Merge LLMs fine-tuned to different tasks without further training to get a resulting model with even better performance.


The "Project Notebook.ipynb" file contains all the code necessary to run the different methods implemented in this project
