{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/dtu/blackhole/02/186978/cache'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/30/d/186978/DL/lib64/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0bd1c1ece90447dbc2944c468667c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae64c12c72e640388e60abe8bd90523f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_math = AutoModelForCausalLM.from_pretrained(\"meta-math/MetaMath-Mistral-7B\")\n",
    "model_bio = AutoModelForCausalLM.from_pretrained(\"BioMistral/BioMistral-7B\")\n",
    "tokenizer_bio = AutoTokenizer.from_pretrained(\"BioMistral/BioMistral-7B\")\n",
    "tokenizer_math = AutoTokenizer.from_pretrained(\"meta-math/MetaMath-Mistral-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "/zhome/30/d/186978/DL/lib64/python3.9/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve the equation x^2 - 4x + 4 = 0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Solve the equation x^2 - 4x + 4 = 0\"\n",
    "inputs = tokenizer_math(input_text, return_tensors=\"pt\")\n",
    "outputs = model_math.generate(**inputs)\n",
    "print(tokenizer_math.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0740f4450687494785c01b1996fbe9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cc3b8408fd4daaa1a0246b3eefe5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "if len(tokenizer_math) < len(base_tokenizer):\n",
    "    tokenizer_math.add_tokens([\"<dummy>\"])\n",
    "    model_math.resize_token_embeddings(len(tokenizer_math))\n",
    "elif len(base_tokenizer) < len(tokenizer_math):\n",
    "    base_tokenizer.add_tokens([\"<dummy>\"])\n",
    "    base_model.resize_token_embeddings(len(base_tokenizer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tokenizer_bio) < len(base_tokenizer):\n",
    "    tokenizer_bio.add_tokens([\"<dummy>\"])\n",
    "    model_bio.resize_token_embeddings(len(tokenizer_bio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32001, 32001, 32001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_math), len(tokenizer_bio), len(base_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_deltas(model, base_model):\n",
    "#     deltas = {}\n",
    "#     for name, param in model.named_parameters():\n",
    "#         base_param = base_model.state_dict()[name]\n",
    "#         deltas[name] = param - base_param\n",
    "#     return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def extract_deltas(model, base_model):\n",
    "    deltas = {}\n",
    "    base_state_dict = base_model.state_dict()\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name in base_state_dict:\n",
    "            base_param = base_state_dict[name]\n",
    "            if param.shape == base_param.shape:\n",
    "                # Detach tensors to avoid gradient issues\n",
    "                deltas[name] = (param.detach() - base_param.detach()).cpu()\n",
    "            else:\n",
    "                print(f\"Skipping {name} due to shape mismatch: {param.shape} vs {base_param.shape}\")\n",
    "        else:\n",
    "            print(f\"Skipping {name} as it is not in the base model\")\n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "deltas_math = extract_deltas(model_math, base_model)\n",
    "deltas_bio = extract_deltas(model_bio, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_dare(deltas, drop_rate):\n",
    "    new_deltas = {}\n",
    "    for name, delta in deltas.items():\n",
    "        # Detach the tensor from the computation graph and convert to NumPy array\n",
    "        mask = np.random.rand(*delta.shape) > drop_rate\n",
    "        mask = torch.tensor(mask, dtype=delta.dtype, device=delta.device)\n",
    "        new_deltas[name] = delta * mask / (1 - drop_rate)\n",
    "    return new_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "drop_rate = 0.9\n",
    "dare_deltas_math = apply_dare(deltas_math, drop_rate)\n",
    "dare_deltas_bio = apply_dare(deltas_bio, drop_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
