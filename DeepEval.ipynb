{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U deepeval\n",
    "# ! pip install numpy\n",
    "# ! pip install transformers\n",
    "# ! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentencepiece\n",
    "# ! pip install tokenizers\n",
    "# ! pip install accelerate\n",
    "# ! pip install bitsandbytes\n",
    "# ! pip install vllm\n",
    "# ! pip install fraction\n",
    "# ! pip install protobuf\n",
    "# ! pip install termtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from deepeval.benchmarks import MMLU, TruthfulQA\n",
    "from deepeval.benchmarks.tasks import MMLUTask, TruthfulQATask\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from typing import List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import termtables as tt\n",
    "import numpy as np\n",
    "\n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {torch_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set HF_HOME to the blackhole-cache directory beforehand \n",
    "blackhole_dir = \"/dtu/blackhole/08/186664/cache\" # (Set to blackhole-cache directory)\n",
    "model_name = \"Mistral Instruct 7B\" # Choose model name from the list below\n",
    "\n",
    "import os\n",
    "os.chdir(blackhole_dir)\n",
    "\n",
    "# List of model names:\n",
    "#--------------------------------------------------------------------------------\n",
    "models = { \n",
    "#   name                  : path\n",
    "    \"Mistral 7B\"          : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral Instruct 7B\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral 7B\"       : \"BioMistral/BioMistral-7B\",\n",
    "    \"MetaMath 7B\"         : \"meta-math/MetaMath-Mistral-7B\",\n",
    "    # \"MetaBioMerge 7B\"     : \"./hub/models--merge--meta--bio--7B\"          ##### Det hedder min model, så kald gerne jeres det samme\n",
    "}\n",
    "#--------------------------------------------------------------------------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper class for models\n",
    "class DeepEvalModelWrapper(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.model.to(torch_device)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        # model = self.load_model()\n",
    "        def find_answer_choice(new_tokens, answer_choices):\n",
    "            if new_tokens in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return new_tokens\n",
    "            for answer_choice in answer_choices:\n",
    "                if new_tokens in answer_choice:\n",
    "                    return answer_choice[0]\n",
    "            return None\n",
    "        \n",
    "        if self.model_name not in [\"MetaMath 7B\"]:\n",
    "            prompt = prompt[:-55]\n",
    "        answer_choices = [prompt[prompt.find(\"A. \"):prompt.find(\"\\n\", prompt.find(\"A. \"))].strip(), \n",
    "                          prompt[prompt.find(\"B. \"):prompt.find(\"\\n\", prompt.find(\"B. \"))].strip(), \n",
    "                            prompt[prompt.find(\"C. \"):prompt.find(\"\\n\", prompt.find(\"C. \"))].strip(), \n",
    "                            prompt[prompt.find(\"D. \"):prompt.find(\"\\n\", prompt.find(\"D. \"))].strip()]\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "        # model.to(torch_device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=(50 if self.model_name in [\"MetaMath 7B\"] else 1), do_sample=True, pad_token_id=self.tokenizer.eos_token_id, temperature=0.0001)\n",
    "        decoded_tokens = self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "        new_tokens = decoded_tokens[len(prompt)+4+(1 if self.model_name in ['MetaMath 7B'] else 0):].strip()\n",
    "        \n",
    "        if self.model_name in [\"MetaMath 7B\"]:\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if \"The answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"The answer is: \")+15:]\n",
    "            elif \"he answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"he answer is: \")+14:]\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            new_tokens = new_tokens.replace(\"</s>\", \"\")\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if new_tokens.strip() == \"\":\n",
    "                return \"\"\n",
    "\n",
    "            if new_tokens[-1] == \".\":\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if \"\\\\text{\" in new_tokens:\n",
    "                new_tokens = new_tokens.replace(\"\\\\text{\", \"\")\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if new_tokens[0] == \"(\" or new_tokens[0] == \"[\":\n",
    "                new_tokens = new_tokens[1:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "        return new_tokens\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        # model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset loaded models to avoid memory issues\n",
    "def free_memory():\n",
    "    # Doesn't work for some reason\n",
    "    try:\n",
    "        del model_wrapped.model\n",
    "        del model_wrapped\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Quick command to load wrapped model\n",
    "def load_model(model_name: str):\n",
    "    free_memory()\n",
    "    model_name_path = models[model_name]\n",
    "    global model_wrapped\n",
    "    global model\n",
    "    global tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_path)\n",
    "    print(\"Model loaded\")\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_path)\n",
    "    print(\"Tokenizer loaded\")\n",
    "    print(\"Wrapping model...\")\n",
    "    model_wrapped = DeepEvalModelWrapper(model_name, model, tokenizer)\n",
    "    print(\"Model wrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c802f1f42ae0455a978f3dc1f2beb3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "Wrapping model...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 40.69 MiB is free. Process 1362707 has 5.55 GiB memory in use. Including non-PyTorch memory, this process has 26.13 GiB memory in use. Of the allocated memory 25.83 GiB is allocated by PyTorch, and 1.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load chosen model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrapping model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m model_wrapped \u001b[38;5;241m=\u001b[39m \u001b[43mDeepEvalModelWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel wrapped\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mDeepEvalModelWrapper.__init__\u001b[0;34m(self, model_name, model, tokenizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/transformers/modeling_utils.py:3157\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[0;32m-> 3157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/dl/lib64/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 40.69 MiB is free. Process 1362707 has 5.55 GiB memory in use. Including non-PyTorch memory, this process has 26.13 GiB memory in use. Of the allocated memory 25.83 GiB is allocated by PyTorch, and 1.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Load chosen model\n",
    "load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(model):\n",
    "    print('### Q1 ###')\n",
    "    print('---')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\\nA. directional selection.\\nB. stabilizing selection.\\nC. sexual selection.\\nD. disruptive selection.\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"Which of the changes below following the start codon in an mRNA would most likely have the greatest deleterious effect?\\nA. a deletion of a single nucleotide\\nB. a deletion of a nucleotide triplet\\nC. a single nucleotide substitution of the nucleotide occupying the first codon position\\nD. a single nucleotide substitution of the nucleotide occupying the third codon position\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: C')\n",
    "    print(model.generate(\"The energy given up by electrons as they move through the electron transport chain is used to\\nA. break down glucose\\nB. make glucose\\nC. produce ATP\\nD. make NADH\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "\n",
    "    print('### Math Questions ###')\n",
    "    print('---')\n",
    "    print('### Q1 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"What is 5 minus 2?\\nA. 3\\nB. 5\\nC. 25\\nD. 50\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"The following are multiple choice questions (with answers) about high school biology.\\n\\nWhich of the following is not a way to form recombinant DNA?\\nA. Translation\\nB. Conjugation\\nC. Specialized transduction\\nD. Transformation\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: D')\n",
    "    print(model.generate(\"If a metamath_7bn P with vertices at (– 2, – 4), (– 4, 1), (–1, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P’, then one of the vertices of P’ is\\nA. (0, – 3)\\nB. (4, 1)\\nC. (2, 2)\\nD. (– 4, –2)\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Q1 ###\n",
      "---\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "B\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: C\n",
      "D\n",
      "### Math Questions ###\n",
      "---\n",
      "### Q1 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: D\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "quick_test(model_wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation benchmarks\n",
    "n_shots = 0\n",
    "\n",
    "mm_tasks_math = [MMLUTask.HIGH_SCHOOL_MATHEMATICS,\n",
    "                 MMLUTask.ABSTRACT_ALGEBRA,\n",
    "                 MMLUTask.MACHINE_LEARNING,\n",
    "                 MMLUTask.ELEMENTARY_MATHEMATICS,\n",
    "                 MMLUTask.COLLEGE_MATHEMATICS,\n",
    "                 MMLUTask.FORMAL_LOGIC,\n",
    "                 MMLUTask.HIGH_SCHOOL_STATISTICS]\n",
    "\n",
    "mm_tasks_bio = [MMLUTask.CLINICAL_KNOWLEDGE,\n",
    "                MMLUTask.MEDICAL_GENETICS,\n",
    "                MMLUTask.ANATOMY,\n",
    "                MMLUTask.PROFESSIONAL_MEDICINE,\n",
    "                MMLUTask.COLLEGE_BIOLOGY,\n",
    "                MMLUTask.COLLEGE_MEDICINE,\n",
    "                MMLUTask.HIGH_SCHOOL_BIOLOGY]\n",
    "\n",
    "mm_tasks_all = mm_tasks_math + mm_tasks_bio\n",
    "\n",
    "tqa_tasks = [TruthfulQATask.SCIENCE]\n",
    "tqa_mode = TruthfulQAMode.MC1 # Use MC1 as a benchmark for pinpoint accuracy and MC2 for depth of understanding.\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "all_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "math_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "bio_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "TQABenchmark = TruthfulQA(\n",
    "    tasks=tqa_tasks,\n",
    "    mode=tqa_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████| 270/270 [01:07<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abstract_algebra: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=abstract_algebra): 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing machine_learning: 100%|█████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:24<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing elementary_mathematics: 100%|███████████████████████████████████████████████████████████████████████████████| 378/378 [01:28<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_mathematics): 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing formal_logic: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:39<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_statistics: 100%|███████████████████████████████████████████████████████████████████████████████| 216/216 [01:17<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clinical_knowledge: 100%|███████████████████████████████████████████████████████████████████████████████████| 265/265 [01:01<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing medical_genetics: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=medical_genetics): 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing anatomy: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:31<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing professional_medicine: 100%|████████████████████████████████████████████████████████████████████████████████| 272/272 [02:12<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=professional_medicine): 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_biology: 100%|██████████████████████████████████████████████████████████████████████████████████████| 144/144 [00:41<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_medicine: 100%|█████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:56<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_biology: 100%|██████████████████████████████████████████████████████████████████████████████████| 310/310 [01:24<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Overall MMLU Accuracy: 0.47130692336171787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = all_benchmark # Choose benchmark from the list above\n",
    "\n",
    "benchmark.evaluate(model=model_wrapped)\n",
    "results = benchmark.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Mistral Instruct 7B:\n",
      "--------------------------------------------------\n",
      "Counting all answers: \n",
      "     Accuracy math score:     0.3579109062980031\n",
      "     Accuracy bio score:      0.5768406004288777\n",
      "     Accuracy overall score:  0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Counting only correctly formatted answers:\n",
      "     Accuracy math score:     0.3579109062980031\n",
      "     Accuracy bio score:      0.5768406004288777\n",
      "     Accuracy overall score:  0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Benchmark dataset sizes:\n",
      "     Number of correctly formatted answers in math:  1302 out of 1302. (100.00000%)\n",
      "     Number of correctly formatted answers in bio:   1399 out of 1399. (100.00000%)\n",
      "     Number of correctly formatted answers overall:  2701 out of 2701. (100.00000%)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_school_mathematics</td>\n",
       "      <td>0.303704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>machine_learning</td>\n",
       "      <td>0.455357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elementary_mathematics</td>\n",
       "      <td>0.341270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>formal_logic</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high_school_statistics</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>0.607547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medical_genetics</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>0.459259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>professional_medicine</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>0.590278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>0.497110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high_school_biology</td>\n",
       "      <td>0.638710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Task     Score\n",
       "0   high_school_mathematics  0.303704\n",
       "1          abstract_algebra  0.330000\n",
       "2          machine_learning  0.455357\n",
       "3    elementary_mathematics  0.341270\n",
       "4       college_mathematics  0.380000\n",
       "5              formal_logic  0.357143\n",
       "6    high_school_statistics  0.407407\n",
       "7        clinical_knowledge  0.607547\n",
       "8          medical_genetics  0.620000\n",
       "9                   anatomy  0.459259\n",
       "10    professional_medicine  0.562500\n",
       "11          college_biology  0.590278\n",
       "12         college_medicine  0.497110\n",
       "13      high_school_biology  0.638710"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all results\n",
    "print(f'Scores for {model_wrapped.get_model_name()}:')\n",
    "\n",
    "math_rows = 0\n",
    "for task in mm_tasks_math:\n",
    "    math_rows += len(math_benchmark.load_benchmark_dataset(task))\n",
    "results_math = results.iloc[:math_rows]\n",
    "results_bio = results.iloc[math_rows:]\n",
    "\n",
    "math_mean_score = results_math['Correct'].mean()\n",
    "bio_mean_score = results_bio['Correct'].mean()\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Counting all answers: \")\n",
    "print(\"     Accuracy math score:     \" + str(math_mean_score))\n",
    "print(\"     Accuracy bio score:      \" + str(bio_mean_score))\n",
    "print(\"     Accuracy overall score:  \" + str(benchmark.overall_score))\n",
    "\n",
    "## Counting only correctly formatted answers\n",
    "\n",
    "results_correct_format = results[results['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_all_correct_format = results_correct_format['Correct'].mean()\n",
    "\n",
    "results_math_correct_format = results_math[results_math['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_math_correct_format = results_math_correct_format['Correct'].mean()\n",
    "\n",
    "results_bio_correct_format = results_bio[results_bio['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_bio_correct_format = results_bio_correct_format['Correct'].mean()\n",
    "print(\"-\"*50)\n",
    "print(\"Counting only correctly formatted answers:\")\n",
    "print(\"     Accuracy math score:     \" + str(acc_math_correct_format))\n",
    "print(\"     Accuracy bio score:      \" + str(acc_bio_correct_format))\n",
    "print(\"     Accuracy overall score:  \" + str(acc_all_correct_format))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Benchmark dataset sizes:\")\n",
    "print(\"     Number of correctly formatted answers in math:  \" + str(len(results_math_correct_format)) + \" out of \" + str(len(results_math)) + f\". ({100*len(results_math_correct_format)/len(results_math):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers in bio:   \" + str(len(results_bio_correct_format)) + \" out of \" + str(len(results_bio)) + f\". ({100*len(results_bio_correct_format)/len(results_bio):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers overall:  \" + str(len(results_correct_format)) + \" out of \" + str(len(results)) + f\". ({100*len(results_correct_format)/len(results):.5f}%)\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "benchmark.task_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       A\n",
      "1       B\n",
      "2       A\n",
      "3       B\n",
      "4       B\n",
      "       ..\n",
      "2696    B\n",
      "2697    B\n",
      "2698    D\n",
      "2699    B\n",
      "2700    A\n",
      "Name: Prediction, Length: 2701, dtype: object\n",
      "A positive integer n is called “powerful” if, for every prime factor p of n, p^2 is also a factor of n. An example of a powerful number is\n",
      "A. 392\n",
      "B. 336\n",
      "C. 300\n",
      "D. 297\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_rows\", 12)\n",
    "print(results['Prediction'])\n",
    "print(results['Input'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing high_school_mathematics: 100%|█████| 270/270 [00:40<00:00,  6.60it/s]\n",
      "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
      "Processing abstract_algebra: 100%|████████████| 100/100 [00:13<00:00,  7.24it/s]\n",
      "MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
      "Processing machine_learning: 100%|████████████| 112/112 [00:17<00:00,  6.49it/s]\n",
      "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
      "Processing elementary_mathematics: 100%|██████| 378/378 [00:55<00:00,  6.85it/s]\n",
      "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
      "Processing college_mathematics: 100%|█████████| 100/100 [00:15<00:00,  6.30it/s]\n",
      "MMLU Task Accuracy (task=college_mathematics): 0.38\n",
      "Processing formal_logic: 100%|████████████████| 126/126 [00:24<00:00,  5.10it/s]\n",
      "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
      "Processing high_school_statistics: 100%|██████| 216/216 [00:45<00:00,  4.78it/s]\n",
      "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
      "Processing clinical_knowledge: 100%|██████████| 265/265 [00:37<00:00,  7.06it/s]\n",
      "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.38it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.62\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.01it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
      "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.32it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
      "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.15it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
      "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.00it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [00:51<00:00,  6.07it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Overall MMLU Accuracy: 0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Overall score for Mistral 7B Instruct on all tasks: 0.47130692336171787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Instruct\n",
    "print(\"\"\"\n",
    "Processing high_school_mathematics: 100%|█████| 270/270 [00:40<00:00,  6.60it/s]\n",
    "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
    "Processing abstract_algebra: 100%|████████████| 100/100 [00:13<00:00,  7.24it/s]\n",
    "MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
    "Processing machine_learning: 100%|████████████| 112/112 [00:17<00:00,  6.49it/s]\n",
    "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
    "Processing elementary_mathematics: 100%|██████| 378/378 [00:55<00:00,  6.85it/s]\n",
    "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
    "Processing college_mathematics: 100%|█████████| 100/100 [00:15<00:00,  6.30it/s]\n",
    "MMLU Task Accuracy (task=college_mathematics): 0.38\n",
    "Processing formal_logic: 100%|████████████████| 126/126 [00:24<00:00,  5.10it/s]\n",
    "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
    "Processing high_school_statistics: 100%|██████| 216/216 [00:45<00:00,  4.78it/s]\n",
    "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
    "Processing clinical_knowledge: 100%|██████████| 265/265 [00:37<00:00,  7.06it/s]\n",
    "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.38it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.62\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.01it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
    "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.32it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
    "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.15it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
    "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.00it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [00:51<00:00,  6.07it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "Overall MMLU Accuracy: 0.47130692336171787\n",
    "--------------------------------------------------\n",
    "Overall score for Mistral 7B Instruct on all tasks: 0.47130692336171787\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [00:50<00:00,  6.08it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.39it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.67\n",
      "Processing virology: 100%|████████████████████| 166/166 [00:22<00:00,  7.32it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.4457831325301205\n",
      "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.33it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [00:46<00:00,  6.54it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.6045751633986928\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.00it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
      "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.01it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
      "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.12it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
      "Overall MMLU Accuracy: 0.5759651307596513\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5759651307596513\n",
      "--------------------------------------------------\n",
      "3-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [02:42<00:00,  1.91it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6516129032258065\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:50<00:00,  1.99it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.65\n",
      "Processing virology: 100%|████████████████████| 166/166 [01:23<00:00,  1.99it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.4819277108433735\n",
      "Processing professional_medicine: 100%|███████| 272/272 [02:57<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [02:38<00:00,  1.93it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.5980392156862745\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [01:08<00:00,  1.97it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
      "Processing college_medicine: 100%|████████████| 173/173 [01:38<00:00,  1.76it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5953757225433526\n",
      "Processing college_biology: 100%|█████████████| 144/144 [01:16<00:00,  1.89it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.5833333333333334\n",
      "Overall MMLU Accuracy: 0.5821917808219178\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5821917808219178\n",
      "--------------------------------------------------\n",
      "5-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [03:27<00:00,  1.49it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6612903225806451\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [01:04<00:00,  1.54it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.64\n",
      "Processing virology: 100%|████████████████████| 166/166 [01:48<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.45180722891566266\n",
      "Processing professional_medicine: 100%|███████| 272/272 [03:43<00:00,  1.22it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [03:22<00:00,  1.51it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.5784313725490197\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [01:28<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
      "Processing college_medicine: 100%|████████████| 173/173 [02:02<00:00,  1.42it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5895953757225434\n",
      "Processing college_biology: 100%|█████████████| 144/144 [01:36<00:00,  1.49it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.6041666666666666\n",
      "Overall MMLU Accuracy: 0.5778331257783312\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5778331257783312 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BioMistral 7B\n",
    "print(\"\"\"\n",
    "0-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [00:50<00:00,  6.08it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.39it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.67\n",
    "Processing virology: 100%|████████████████████| 166/166 [00:22<00:00,  7.32it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.4457831325301205\n",
    "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.33it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [00:46<00:00,  6.54it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.6045751633986928\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.00it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
    "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.01it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
    "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.12it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
    "Overall MMLU Accuracy: 0.5759651307596513\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5759651307596513\n",
    "--------------------------------------------------\n",
    "3-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [02:42<00:00,  1.91it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6516129032258065\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:50<00:00,  1.99it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.65\n",
    "Processing virology: 100%|████████████████████| 166/166 [01:23<00:00,  1.99it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.4819277108433735\n",
    "Processing professional_medicine: 100%|███████| 272/272 [02:57<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [02:38<00:00,  1.93it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.5980392156862745\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [01:08<00:00,  1.97it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
    "Processing college_medicine: 100%|████████████| 173/173 [01:38<00:00,  1.76it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5953757225433526\n",
    "Processing college_biology: 100%|█████████████| 144/144 [01:16<00:00,  1.89it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.5833333333333334\n",
    "Overall MMLU Accuracy: 0.5821917808219178\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5821917808219178\n",
    "--------------------------------------------------\n",
    "5-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [03:27<00:00,  1.49it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6612903225806451\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [01:04<00:00,  1.54it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.64\n",
    "Processing virology: 100%|████████████████████| 166/166 [01:48<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.45180722891566266\n",
    "Processing professional_medicine: 100%|███████| 272/272 [03:43<00:00,  1.22it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [03:22<00:00,  1.51it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.5784313725490197\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [01:28<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
    "Processing college_medicine: 100%|████████████| 173/173 [02:02<00:00,  1.42it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5895953757225434\n",
    "Processing college_biology: 100%|█████████████| 144/144 [01:36<00:00,  1.49it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.6041666666666666\n",
    "Overall MMLU Accuracy: 0.5778331257783312\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5778331257783312 \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models are evaluated using zero-shot prediction. Numbers are rounded to 5 decimal places, and overall scores are based on correctly formatted answers.\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| \u001b[01mAccuracy Metric\u001b[0m             | \u001b[01mMistral 7B Instruct\u001b[0m | \u001b[01mBioMistral 7B\u001b[0m | \u001b[01mMetaMath 7B\u001b[0m | \u001b[01mBioLoRA 7B\u001b[0m | \u001b[01mMetaLoRA 7B\u001b[0m | \u001b[01mMetaBio 7B\u001b[0m | \u001b[01mMetaBioLoRA 7B\u001b[0m |\n",
      "+=============================+=====================+===============+=============+============+=============+============+================+\n",
      "| Correctly Formatted Answers | 100%                | 99.9%         | 98.6%       | 100%       | 99.4%       | 100%       | 99.9%          |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Overall Score               | 0.47131             | 0.48073       | \u001b[31m0.37566\u001b[0m     | 0.46057    | 0.40395     | \u001b[32m0.49500\u001b[0m    | 0.4472         |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Overall Score - MATH        | 0.35791             | 0.35951       | \u001b[31m0.26877\u001b[0m     | 0.34332    | 0.30692     | \u001b[32m0.36252\u001b[0m    | 0.34           |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Overall Score - BIO         | 0.57684             | 0.59328       | \u001b[31m0.47244\u001b[0m     | 0.56969    | 0.49321     | \u001b[32m0.61830\u001b[0m    | 0.54682        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Math                     | 0.3037              | \u001b[32m0.31482\u001b[0m       | \u001b[31m0.21111\u001b[0m     | 0.27037    | 0.27778     | 0.30741    | 0.28519        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Abstract Algebra            | \u001b[32m0.33000\u001b[0m             | 0.3           | \u001b[31m0.25000\u001b[0m     | 0.3        | 0.27        | 0.3        | 0.31           |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Machine Learning            | 0.45536             | 0.46429       | 0.40179     | 0.45536    | 0.42857     | \u001b[32m0.48214\u001b[0m    | \u001b[31m0.38393\u001b[0m        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Elementary Math             | 0.34127             | \u001b[32m0.34656\u001b[0m       | \u001b[31m0.29101\u001b[0m     | 0.33069    | 0.32275     | 0.34921    | 0.34392        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Math                | \u001b[32m0.38000\u001b[0m             | 0.35          | \u001b[31m0.18000\u001b[0m     | 0.35       | 0.24        | 0.36       | 0.29           |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Formal Logic                | 0.35714             | 0.33333       | \u001b[31m0.27778\u001b[0m     | 0.35714    | 0.30159     | 0.35714    | \u001b[32m0.41270\u001b[0m        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Stats                    | 0.40741             | \u001b[32m0.42593\u001b[0m       | \u001b[31m0.24074\u001b[0m     | 0.40741    | 0.28241     | \u001b[32m0.42593\u001b[0m    | 0.37037        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Clinical KG                 | 0.60755             | 0.61887       | \u001b[31m0.46793\u001b[0m     | 0.62264    | 0.50943     | \u001b[32m0.64528\u001b[0m    | 0.53962        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Medical Genetics            | 0.62                | 0.67          | \u001b[31m0.49000\u001b[0m     | 0.61       | 0.53        | \u001b[32m0.68000\u001b[0m    | 0.62           |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Anatomy                     | 0.45926             | 0.4963        | 0.48889     | 0.46667    | \u001b[31m0.45185\u001b[0m     | \u001b[32m0.51111\u001b[0m    | 0.47407        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Professional Medicine       | 0.5625              | 0.5625        | \u001b[31m0.41544\u001b[0m     | 0.51838    | 0.47059     | \u001b[32m0.57721\u001b[0m    | 0.54412        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Biology             | 0.59028             | 0.61806       | 0.50694     | 0.5625     | \u001b[31m0.49306\u001b[0m     | \u001b[32m0.62500\u001b[0m    | 0.5625         |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Medicine            | 0.49711             | 0.54913       | \u001b[31m0.39306\u001b[0m     | 0.51445    | \u001b[31m0.39306\u001b[0m     | \u001b[32m0.58960\u001b[0m    | 0.45665        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Biology                  | 0.63871             | 0.63871       | \u001b[31m0.53871\u001b[0m     | 0.63548    | 0.56129     | \u001b[32m0.67097\u001b[0m    | 0.60645        |\n",
      "+-----------------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# header = ['Model', 'Overall Score (0-shot)', 'Overall Score (3-shot)', 'Overall Score (5-shot)', 'Overall Score - MATH', 'Overall Score - BIO']\n",
    "# data = [\n",
    "#     # [metamath_7b.get_model_name(), math_benchmark.overall_score],\n",
    "#     ['Mistral 7B Instruct', 0.47131, '-', '-', '-', '-'],\n",
    "#     ['BioMistral 7B', 0.57597, 0.58219, 0.57783, '-', '-'],\n",
    "#     ['-', '-', '-', '-', '-', '-']\n",
    "# ]\n",
    "\n",
    "header = ['\\033[01mAccuracy Metric\\033[0m', '\\033[01mMistral 7B Instruct\\033[0m', '\\033[01mBioMistral 7B\\033[0m', '\\033[01mMetaMath 7B\\033[0m', '\\033[01mBioLoRA 7B\\033[0m', '\\033[01mMetaLoRA 7B\\033[0m', '\\033[01mMetaBio 7B\\033[0m', '\\033[01mMetaBioLoRA 7B\\033[0m']\n",
    "data = [\n",
    "    ['Correctly Formatted Answers', '100%', '99.9%', '98.6%', '100%', '99.4%', '100%', '99.9%'],\n",
    "    ['Overall Score', 0.47131, 0.48073, '\\033[31m0.37566\\033[0m', 0.46057, 0.40395, '\\033[32m0.49500\\033[0m', 0.44720],\n",
    "    ['Overall Score - MATH', 0.35791, 0.35951, '\\033[31m0.26877\\033[0m', 0.34332, 0.30692, '\\033[32m0.36252\\033[0m', 0.34000],\n",
    "    ['Overall Score - BIO', 0.57684, 0.59328, '\\033[31m0.47244\\033[0m', 0.56969, 0.49321, '\\033[32m0.61830\\033[0m', 0.54682],\n",
    "    ['HS Math', 0.30370, '\\033[32m0.31482\\033[0m', '\\033[31m0.21111\\033[0m', 0.27037, 0.27778, 0.30741, 0.28519],\n",
    "    ['Abstract Algebra', '\\033[32m0.33000\\033[0m', 0.30000, '\\033[31m0.25000\\033[0m', 0.30000, 0.27000, 0.30000, 0.31000],\n",
    "    ['Machine Learning', 0.45536, 0.46429, 0.40179, 0.45536, 0.42857, '\\033[32m0.48214\\033[0m', '\\033[31m0.38393\\033[0m'],\n",
    "    ['Elementary Math', 0.34127, '\\033[32m0.34656\\033[0m', '\\033[31m0.29101\\033[0m', 0.33069, 0.32275, 0.34921, 0.34392],\n",
    "    ['College Math', '\\033[32m0.38000\\033[0m', 0.35000, '\\033[31m0.18000\\033[0m', 0.35000, 0.24000, 0.36000, 0.29000],\n",
    "    ['Formal Logic', 0.35714, 0.33333, '\\033[31m0.27778\\033[0m', 0.35714, 0.30159, 0.35714, '\\033[32m0.41270\\033[0m'],\n",
    "    ['HS Stats', 0.40741, '\\033[32m0.42593\\033[0m', '\\033[31m0.24074\\033[0m', 0.40741, 0.28241, '\\033[32m0.42593\\033[0m', 0.37037],\n",
    "    ['Clinical KG', 0.60755, 0.61887, '\\033[31m0.46793\\033[0m', 0.62264, 0.50943, '\\033[32m0.64528\\033[0m', 0.53962],\n",
    "    ['Medical Genetics', 0.62000, 0.67000, '\\033[31m0.49000\\033[0m', 0.61000, 0.53000, '\\033[32m0.68000\\033[0m', 0.62000],\n",
    "    ['Anatomy', 0.45926, 0.49630, 0.48889, 0.46667, '\\033[31m0.45185\\033[0m', '\\033[32m0.51111\\033[0m', 0.47407],\n",
    "    ['Professional Medicine', 0.56250, 0.56250, '\\033[31m0.41544\\033[0m', 0.51838, 0.47059, '\\033[32m0.57721\\033[0m', 0.54412],\n",
    "    ['College Biology', 0.59028, 0.61806, 0.50694, 0.56250, '\\033[31m0.49306\\033[0m', '\\033[32m0.62500\\033[0m', 0.56250],\n",
    "    ['College Medicine', 0.49711, 0.54913, '\\033[31m0.39306\\033[0m', 0.51445, '\\033[31m0.39306\\033[0m', '\\033[32m0.58960\\033[0m', 0.45665],\n",
    "    ['HS Biology', 0.63871, 0.63871, '\\033[31m0.53871\\033[0m', 0.63548, 0.56129, '\\033[32m0.67097\\033[0m', 0.60645]\n",
    "]\n",
    "\n",
    "print('All models are evaluated using zero-shot prediction. Numbers are rounded to 5 decimal places, and overall scores are based on correctly formatted answers.')\n",
    "tt.print(\n",
    "    data,\n",
    "    header=header,\n",
    "    style=tt.styles.ascii_thin_double,\n",
    "    padding=(0, 1),\n",
    "    #alignment=\"lcr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH\n",
      "0.36784\n",
      "BIO\n",
      "0.56791\n"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Instruct\n",
    "# MATH\n",
    "# MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
    "# MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
    "# MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
    "# MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
    "# MMLU Task Accuracy (task=college_mathematics): 0.38\n",
    "# MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
    "# MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
    "# BIO\n",
    "# MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
    "# MMLU Task Accuracy (task=medical_genetics): 0.62\n",
    "# MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
    "# MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
    "# MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
    "# MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
    "# MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "\n",
    "print('MATH')\n",
    "print(np.round((0.3037037037037037 + 0.33 + 0.45535714285714285 + 0.3412698412698413 + 0.38 + 0.35714285714285715 + 0.4074074074074074) / 7, 5))\n",
    "print('BIO')\n",
    "print(np.round((0.6075471698113207 + 0.62 + 0.45925925925925926 + 0.5625 + 0.5902777777777778 + 0.49710982658959535 + 0.6387096774193548) / 7, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH\n",
      "0.36213\n",
      "BIO\n",
      "0.59179\n"
     ]
    }
   ],
   "source": [
    "# BioMistral 7B\n",
    "# MATH\n",
    "# MMLU Task Accuracy (task=high_school_mathematics): 0.3148148148148148\n",
    "# MMLU Task Accuracy (task=abstract_algebra): 0.3\n",
    "# MMLU Task Accuracy (task=machine_learning): 0.4642857142857143\n",
    "# MMLU Task Accuracy (task=elementary_mathematics): 0.34656084656084657\n",
    "# MMLU Task Accuracy (task=college_mathematics): 0.35\n",
    "# MMLU Task Accuracy (task=formal_logic): 0.3333333333333333\n",
    "# MMLU Task Accuracy (task=high_school_statistics): 0.42592592592592593\n",
    "# BIO\n",
    "# MMLU Task Accuracy (task=clinical_knowledge): 0.6188679245283019\n",
    "# MMLU Task Accuracy (task=medical_genetics): 0.67\n",
    "# MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
    "# MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
    "# MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
    "# MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
    "# MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "\n",
    "print('MATH')\n",
    "print(np.round((0.3148148148148148 + 0.3 + 0.4642857142857143 + 0.34656084656084657 + 0.35 + 0.3333333333333333 + 0.42592592592592593) / 7, 5))\n",
    "print('BIO')\n",
    "print(np.round((0.6188679245283019 + 0.67 + 0.4962962962962963 + 0.5514705882352942 + 0.6180555555555556 + 0.5491329479768786 + 0.6387096774193548) / 7, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36883"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MetaBio\n",
    "# MATH\n",
    "np.round((0.307407 + 0.300000 + 0.482143 + 0.349206 + 0.360000 + 0.357143 + 0.425926) / 7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
