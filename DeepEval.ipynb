{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U deepeval\n",
    "# ! pip install numpy\n",
    "# ! pip install transformers\n",
    "# ! pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install sentencepiece\n",
    "# ! pip install tokenizers\n",
    "# ! pip install accelerate\n",
    "# ! pip install bitsandbytes\n",
    "# ! pip install vllm\n",
    "# ! pip install fraction\n",
    "# ! pip install protobuf\n",
    "# ! pip install termtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from deepeval.benchmarks import MMLU, TruthfulQA\n",
    "from deepeval.benchmarks.tasks import MMLUTask, TruthfulQATask\n",
    "from deepeval.benchmarks.modes import TruthfulQAMode\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from typing import List\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import termtables as tt\n",
    "import numpy as np\n",
    "\n",
    "torch_device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {torch_device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set HF_HOME to the blackhole-cache directory beforehand \n",
    "blackhole_dir = \"/dtu/blackhole/06/187238/cache\" # (Set to blackhole-cache directory)\n",
    "model_name = \"Mistral Instruct 7B\" # Choose model name from the list below\n",
    "\n",
    "import os\n",
    "os.chdir(blackhole_dir)\n",
    "\n",
    "# List of model names:\n",
    "#--------------------------------------------------------------------------------\n",
    "models = { \n",
    "#   name                  : path\n",
    "    \"Mistral 7B\"          : \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"Mistral Instruct 7B\" : \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    \"BioMistral 7B\"       : \"BioMistral/BioMistral-7B\",\n",
    "    \"MetaMath 7B\"         : \"meta-math/MetaMath-Mistral-7B\",\n",
    "    \"MetaBioMerge 7B\"     : \"./hub/models--merge--meta--bio--7B\"          ##### Det hedder min model, så kald gerne jeres det samme\n",
    "}\n",
    "#--------------------------------------------------------------------------------        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define wrapper class for models\n",
    "class DeepEvalModelWrapper(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.model.to(torch_device)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        # model = self.load_model()\n",
    "        def find_answer_choice(new_tokens, answer_choices):\n",
    "            if new_tokens in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "                return new_tokens\n",
    "            for answer_choice in answer_choices:\n",
    "                if new_tokens in answer_choice:\n",
    "                    return answer_choice[0]\n",
    "            return None\n",
    "        \n",
    "        if self.model_name not in [\"MetaMath 7B\"]:\n",
    "            prompt = prompt[:-55]\n",
    "        answer_choices = [prompt[prompt.find(\"A. \"):prompt.find(\"\\n\", prompt.find(\"A. \"))].strip(), \n",
    "                          prompt[prompt.find(\"B. \"):prompt.find(\"\\n\", prompt.find(\"B. \"))].strip(), \n",
    "                            prompt[prompt.find(\"C. \"):prompt.find(\"\\n\", prompt.find(\"C. \"))].strip(), \n",
    "                            prompt[prompt.find(\"D. \"):prompt.find(\"\\n\", prompt.find(\"D. \"))].strip()]\n",
    "\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(torch_device)\n",
    "        # model.to(torch_device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=(50 if self.model_name in [\"MetaMath 7B\"] else 1), do_sample=True, pad_token_id=self.tokenizer.eos_token_id, temperature=0.0001)\n",
    "        decoded_tokens = self.tokenizer.batch_decode(generated_ids)[0]\n",
    "\n",
    "        new_tokens = decoded_tokens[len(prompt)+4+(1 if self.model_name in ['MetaMath 7B'] else 0):].strip()\n",
    "        \n",
    "        if self.model_name in [\"MetaMath 7B\"]:\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if \"The answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"The answer is: \")+15:]\n",
    "            elif \"he answer is: \" in new_tokens:\n",
    "                new_tokens = new_tokens[new_tokens.find(\"he answer is: \")+14:]\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            new_tokens = new_tokens.replace(\"</s>\", \"\")\n",
    "            fac = find_answer_choice(new_tokens, answer_choices)\n",
    "            if fac:\n",
    "                return fac\n",
    "            if new_tokens.strip() == \"\":\n",
    "                return \"\"\n",
    "\n",
    "            if new_tokens[-1] == \".\":\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if \"\\\\text{\" in new_tokens:\n",
    "                new_tokens = new_tokens.replace(\"\\\\text{\", \"\")\n",
    "                new_tokens = new_tokens[:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "            if new_tokens[0] == \"(\" or new_tokens[0] == \"[\":\n",
    "                new_tokens = new_tokens[1:-1]\n",
    "                fac = find_answer_choice(new_tokens, answer_choices)\n",
    "                if fac:\n",
    "                    return fac\n",
    "        return new_tokens\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    # This is optional.\n",
    "    def batch_generate(self, promtps: List[str]) -> List[str]:\n",
    "        # model = self.load_model()\n",
    "        device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "        model_inputs = self.tokenizer(promtps, return_tensors=\"pt\").to(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "        return self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset loaded models to avoid memory issues\n",
    "def free_memory():\n",
    "    # Doesn't work for some reason\n",
    "    try:\n",
    "        del model_wrapped.model\n",
    "        del model_wrapped\n",
    "        del model\n",
    "        del tokenizer\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "# Quick command to load wrapped model\n",
    "def load_model(model_name: str):\n",
    "    free_memory()\n",
    "    model_name_path = models[model_name]\n",
    "    global model_wrapped\n",
    "    global model\n",
    "    global tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_path)\n",
    "    print(\"Model loaded\")\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_path)\n",
    "    print(\"Tokenizer loaded\")\n",
    "    print(\"Wrapping model...\")\n",
    "    model_wrapped = DeepEvalModelWrapper(model_name, model, tokenizer)\n",
    "    print(\"Model wrapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fbce960e9240889b529c16b997488c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loading tokenizer...\n",
      "Tokenizer loaded\n",
      "Wrapping model...\n",
      "Model wrapped\n"
     ]
    }
   ],
   "source": [
    "# Load chosen model\n",
    "load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test(model):\n",
    "    print('### Q1 ###')\n",
    "    print('---')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"In a population of giraffes, an environmental change occurs that favors individuals that are tallest. As a result, more of the taller individuals are able to obtain nutrients and survive to pass along their genetic information. This is an example of\\nA. directional selection.\\nB. stabilizing selection.\\nC. sexual selection.\\nD. disruptive selection.\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"Which of the changes below following the start codon in an mRNA would most likely have the greatest deleterious effect?\\nA. a deletion of a single nucleotide\\nB. a deletion of a nucleotide triplet\\nC. a single nucleotide substitution of the nucleotide occupying the first codon position\\nD. a single nucleotide substitution of the nucleotide occupying the third codon position\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: C')\n",
    "    print(model.generate(\"The energy given up by electrons as they move through the electron transport chain is used to\\nA. break down glucose\\nB. make glucose\\nC. produce ATP\\nD. make NADH\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "\n",
    "    print('### Math Questions ###')\n",
    "    print('---')\n",
    "    print('### Q1 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"What is 5 minus 2?\\nA. 3\\nB. 5\\nC. 25\\nD. 50\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q2 ###')\n",
    "    print('Correct: A')\n",
    "    print(model.generate(\"The following are multiple choice questions (with answers) about high school biology.\\n\\nWhich of the following is not a way to form recombinant DNA?\\nA. Translation\\nB. Conjugation\\nC. Specialized transduction\\nD. Transformation\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))\n",
    "    print('---')\n",
    "    print('### Q3 ###')\n",
    "    print('Correct: D')\n",
    "    print(model.generate(\"If a metamath_7bn P with vertices at (– 2, – 4), (– 4, 1), (–1, 4), (2, 4), and (3, 0) is reflected across the line y = x to get a new pentagon, P’, then one of the vertices of P’ is\\nA. (0, – 3)\\nB. (4, 1)\\nC. (2, 2)\\nD. (– 4, –2)\\nAnswer:\\n\\nOutput 'A', 'B', 'C', or 'D'. Full answer not needed.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Q1 ###\n",
      "---\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "B\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: C\n",
      "D\n",
      "### Math Questions ###\n",
      "---\n",
      "### Q1 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q2 ###\n",
      "Correct: A\n",
      "A\n",
      "---\n",
      "### Q3 ###\n",
      "Correct: D\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "quick_test(model_wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation benchmarks\n",
    "n_shots = 0\n",
    "\n",
    "mm_tasks_math = [MMLUTask.HIGH_SCHOOL_MATHEMATICS,\n",
    "                 MMLUTask.ABSTRACT_ALGEBRA,\n",
    "                 MMLUTask.MACHINE_LEARNING,\n",
    "                 MMLUTask.ELEMENTARY_MATHEMATICS,\n",
    "                 MMLUTask.COLLEGE_MATHEMATICS,\n",
    "                 MMLUTask.FORMAL_LOGIC,\n",
    "                 MMLUTask.HIGH_SCHOOL_STATISTICS]\n",
    "\n",
    "mm_tasks_bio = [MMLUTask.CLINICAL_KNOWLEDGE,\n",
    "                MMLUTask.MEDICAL_GENETICS,\n",
    "                MMLUTask.ANATOMY,\n",
    "                MMLUTask.PROFESSIONAL_MEDICINE,\n",
    "                MMLUTask.COLLEGE_BIOLOGY,\n",
    "                MMLUTask.COLLEGE_MEDICINE,\n",
    "                MMLUTask.HIGH_SCHOOL_BIOLOGY]\n",
    "\n",
    "mm_tasks_all = mm_tasks_math + mm_tasks_bio\n",
    "\n",
    "tqa_tasks = [TruthfulQATask.SCIENCE]\n",
    "tqa_mode = TruthfulQAMode.MC1 # Use MC1 as a benchmark for pinpoint accuracy and MC2 for depth of understanding.\n",
    "\n",
    "# Define benchmark with specific tasks and shots\n",
    "all_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "math_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "bio_benchmark = MMLU(\n",
    "    tasks=mm_tasks_all,\n",
    "    n_shots=n_shots\n",
    ")\n",
    "\n",
    "TQABenchmark = TruthfulQA(\n",
    "    tasks=tqa_tasks,\n",
    "    mode=tqa_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████| 270/270 [01:07<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing abstract_algebra: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=abstract_algebra): 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing machine_learning: 100%|█████████████████████████████████████████████████████████████████████████████████████| 112/112 [00:24<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing elementary_mathematics: 100%|███████████████████████████████████████████████████████████████████████████████| 378/378 [01:28<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_mathematics: 100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_mathematics): 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing formal_logic: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:39<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_statistics: 100%|███████████████████████████████████████████████████████████████████████████████| 216/216 [01:17<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clinical_knowledge: 100%|███████████████████████████████████████████████████████████████████████████████████| 265/265 [01:01<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing medical_genetics: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=medical_genetics): 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing anatomy: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:31<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing professional_medicine: 100%|████████████████████████████████████████████████████████████████████████████████| 272/272 [02:12<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=professional_medicine): 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_biology: 100%|██████████████████████████████████████████████████████████████████████████████████████| 144/144 [00:41<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing college_medicine: 100%|█████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:56<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing high_school_biology: 100%|██████████████████████████████████████████████████████████████████████████████████| 310/310 [01:24<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Overall MMLU Accuracy: 0.47130692336171787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = all_benchmark # Choose benchmark from the list above\n",
    "\n",
    "benchmark.evaluate(model=model_wrapped)\n",
    "results = benchmark.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Mistral Instruct 7B:\n",
      "--------------------------------------------------\n",
      "Counting all answers: \n",
      "     Accuracy math score:     0.3579109062980031\n",
      "     Accuracy bio score:      0.5768406004288777\n",
      "     Accuracy overall score:  0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Counting only correctly formatted answers:\n",
      "     Accuracy math score:     0.3579109062980031\n",
      "     Accuracy bio score:      0.5768406004288777\n",
      "     Accuracy overall score:  0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Benchmark dataset sizes:\n",
      "     Number of correctly formatted answers in math:  1302 out of 1302. (100.00000%)\n",
      "     Number of correctly formatted answers in bio:   1399 out of 1399. (100.00000%)\n",
      "     Number of correctly formatted answers overall:  2701 out of 2701. (100.00000%)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_school_mathematics</td>\n",
       "      <td>0.303704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abstract_algebra</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>machine_learning</td>\n",
       "      <td>0.455357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elementary_mathematics</td>\n",
       "      <td>0.341270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>college_mathematics</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>formal_logic</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high_school_statistics</td>\n",
       "      <td>0.407407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>clinical_knowledge</td>\n",
       "      <td>0.607547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>medical_genetics</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anatomy</td>\n",
       "      <td>0.459259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>professional_medicine</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>college_biology</td>\n",
       "      <td>0.590278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college_medicine</td>\n",
       "      <td>0.497110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high_school_biology</td>\n",
       "      <td>0.638710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Task     Score\n",
       "0   high_school_mathematics  0.303704\n",
       "1          abstract_algebra  0.330000\n",
       "2          machine_learning  0.455357\n",
       "3    elementary_mathematics  0.341270\n",
       "4       college_mathematics  0.380000\n",
       "5              formal_logic  0.357143\n",
       "6    high_school_statistics  0.407407\n",
       "7        clinical_knowledge  0.607547\n",
       "8          medical_genetics  0.620000\n",
       "9                   anatomy  0.459259\n",
       "10    professional_medicine  0.562500\n",
       "11          college_biology  0.590278\n",
       "12         college_medicine  0.497110\n",
       "13      high_school_biology  0.638710"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all results\n",
    "print(f'Scores for {model_wrapped.get_model_name()}:')\n",
    "\n",
    "math_rows = 0\n",
    "for task in mm_tasks_math:\n",
    "    math_rows += len(math_benchmark.load_benchmark_dataset(task))\n",
    "results_math = results.iloc[:math_rows]\n",
    "results_bio = results.iloc[math_rows:]\n",
    "\n",
    "math_mean_score = results_math['Correct'].mean()\n",
    "bio_mean_score = results_bio['Correct'].mean()\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Counting all answers: \")\n",
    "print(\"     Accuracy math score:     \" + str(math_mean_score))\n",
    "print(\"     Accuracy bio score:      \" + str(bio_mean_score))\n",
    "print(\"     Accuracy overall score:  \" + str(benchmark.overall_score))\n",
    "\n",
    "## Counting only correctly formatted answers\n",
    "\n",
    "results_correct_format = results[results['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_all_correct_format = results_correct_format['Correct'].mean()\n",
    "\n",
    "results_math_correct_format = results_math[results_math['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_math_correct_format = results_math_correct_format['Correct'].mean()\n",
    "\n",
    "results_bio_correct_format = results_bio[results_bio['Prediction'].isin([\"A\", \"B\", \"C\", \"D\"])]\n",
    "acc_bio_correct_format = results_bio_correct_format['Correct'].mean()\n",
    "print(\"-\"*50)\n",
    "print(\"Counting only correctly formatted answers:\")\n",
    "print(\"     Accuracy math score:     \" + str(acc_math_correct_format))\n",
    "print(\"     Accuracy bio score:      \" + str(acc_bio_correct_format))\n",
    "print(\"     Accuracy overall score:  \" + str(acc_all_correct_format))\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Benchmark dataset sizes:\")\n",
    "print(\"     Number of correctly formatted answers in math:  \" + str(len(results_math_correct_format)) + \" out of \" + str(len(results_math)) + f\". ({100*len(results_math_correct_format)/len(results_math):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers in bio:   \" + str(len(results_bio_correct_format)) + \" out of \" + str(len(results_bio)) + f\". ({100*len(results_bio_correct_format)/len(results_bio):.5f}%)\")\n",
    "print(\"     Number of correctly formatted answers overall:  \" + str(len(results_correct_format)) + \" out of \" + str(len(results)) + f\". ({100*len(results_correct_format)/len(results):.5f}%)\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "benchmark.task_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       A\n",
      "1       B\n",
      "2       A\n",
      "3       B\n",
      "4       B\n",
      "       ..\n",
      "2696    B\n",
      "2697    B\n",
      "2698    D\n",
      "2699    B\n",
      "2700    A\n",
      "Name: Prediction, Length: 2701, dtype: object\n",
      "A positive integer n is called “powerful” if, for every prime factor p of n, p^2 is also a factor of n. An example of a powerful number is\n",
      "A. 392\n",
      "B. 336\n",
      "C. 300\n",
      "D. 297\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_rows\", 12)\n",
    "print(results['Prediction'])\n",
    "print(results['Input'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing high_school_mathematics: 100%|█████| 270/270 [00:40<00:00,  6.60it/s]\n",
      "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
      "Processing abstract_algebra: 100%|████████████| 100/100 [00:13<00:00,  7.24it/s]\n",
      "MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
      "Processing machine_learning: 100%|████████████| 112/112 [00:17<00:00,  6.49it/s]\n",
      "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
      "Processing elementary_mathematics: 100%|██████| 378/378 [00:55<00:00,  6.85it/s]\n",
      "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
      "Processing college_mathematics: 100%|█████████| 100/100 [00:15<00:00,  6.30it/s]\n",
      "MMLU Task Accuracy (task=college_mathematics): 0.38\n",
      "Processing formal_logic: 100%|████████████████| 126/126 [00:24<00:00,  5.10it/s]\n",
      "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
      "Processing high_school_statistics: 100%|██████| 216/216 [00:45<00:00,  4.78it/s]\n",
      "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
      "Processing clinical_knowledge: 100%|██████████| 265/265 [00:37<00:00,  7.06it/s]\n",
      "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.38it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.62\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.01it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
      "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.32it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
      "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.15it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
      "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.00it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [00:51<00:00,  6.07it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Overall MMLU Accuracy: 0.47130692336171787\n",
      "--------------------------------------------------\n",
      "Overall score for Mistral 7B Instruct on all tasks: 0.47130692336171787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Instruct\n",
    "print(\"\"\"\n",
    "Processing high_school_mathematics: 100%|█████| 270/270 [00:40<00:00,  6.60it/s]\n",
    "MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
    "Processing abstract_algebra: 100%|████████████| 100/100 [00:13<00:00,  7.24it/s]\n",
    "MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
    "Processing machine_learning: 100%|████████████| 112/112 [00:17<00:00,  6.49it/s]\n",
    "MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
    "Processing elementary_mathematics: 100%|██████| 378/378 [00:55<00:00,  6.85it/s]\n",
    "MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
    "Processing college_mathematics: 100%|█████████| 100/100 [00:15<00:00,  6.30it/s]\n",
    "MMLU Task Accuracy (task=college_mathematics): 0.38\n",
    "Processing formal_logic: 100%|████████████████| 126/126 [00:24<00:00,  5.10it/s]\n",
    "MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
    "Processing high_school_statistics: 100%|██████| 216/216 [00:45<00:00,  4.78it/s]\n",
    "MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
    "Processing clinical_knowledge: 100%|██████████| 265/265 [00:37<00:00,  7.06it/s]\n",
    "MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.38it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.62\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.01it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
    "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.32it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
    "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.15it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
    "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.00it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [00:51<00:00,  6.07it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "Overall MMLU Accuracy: 0.47130692336171787\n",
    "--------------------------------------------------\n",
    "Overall score for Mistral 7B Instruct on all tasks: 0.47130692336171787\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [00:50<00:00,  6.08it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.39it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.67\n",
      "Processing virology: 100%|████████████████████| 166/166 [00:22<00:00,  7.32it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.4457831325301205\n",
      "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.33it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [00:46<00:00,  6.54it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.6045751633986928\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.00it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
      "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.01it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
      "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.12it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
      "Overall MMLU Accuracy: 0.5759651307596513\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5759651307596513\n",
      "--------------------------------------------------\n",
      "3-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [02:42<00:00,  1.91it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6516129032258065\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [00:50<00:00,  1.99it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.65\n",
      "Processing virology: 100%|████████████████████| 166/166 [01:23<00:00,  1.99it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.4819277108433735\n",
      "Processing professional_medicine: 100%|███████| 272/272 [02:57<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [02:38<00:00,  1.93it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.5980392156862745\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [01:08<00:00,  1.97it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
      "Processing college_medicine: 100%|████████████| 173/173 [01:38<00:00,  1.76it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5953757225433526\n",
      "Processing college_biology: 100%|█████████████| 144/144 [01:16<00:00,  1.89it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.5833333333333334\n",
      "Overall MMLU Accuracy: 0.5821917808219178\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5821917808219178\n",
      "--------------------------------------------------\n",
      "5-shot\n",
      "Processing high_school_biology: 100%|█████████| 310/310 [03:27<00:00,  1.49it/s]\n",
      "MMLU Task Accuracy (task=high_school_biology): 0.6612903225806451\n",
      "Processing medical_genetics: 100%|████████████| 100/100 [01:04<00:00,  1.54it/s]\n",
      "MMLU Task Accuracy (task=medical_genetics): 0.64\n",
      "Processing virology: 100%|████████████████████| 166/166 [01:48<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=virology): 0.45180722891566266\n",
      "Processing professional_medicine: 100%|███████| 272/272 [03:43<00:00,  1.22it/s]\n",
      "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
      "Processing nutrition: 100%|███████████████████| 306/306 [03:22<00:00,  1.51it/s]\n",
      "MMLU Task Accuracy (task=nutrition): 0.5784313725490197\n",
      "Processing anatomy: 100%|█████████████████████| 135/135 [01:28<00:00,  1.53it/s]\n",
      "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
      "Processing college_medicine: 100%|████████████| 173/173 [02:02<00:00,  1.42it/s]\n",
      "MMLU Task Accuracy (task=college_medicine): 0.5895953757225434\n",
      "Processing college_biology: 100%|█████████████| 144/144 [01:36<00:00,  1.49it/s]\n",
      "MMLU Task Accuracy (task=college_biology): 0.6041666666666666\n",
      "Overall MMLU Accuracy: 0.5778331257783312\n",
      "--------------------------------------------------\n",
      "Overall score for BioMistral 7B: 0.5778331257783312 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BioMistral 7B\n",
    "print(\"\"\"\n",
    "0-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [00:50<00:00,  6.08it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:13<00:00,  7.39it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.67\n",
    "Processing virology: 100%|████████████████████| 166/166 [00:22<00:00,  7.32it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.4457831325301205\n",
    "Processing professional_medicine: 100%|███████| 272/272 [01:21<00:00,  3.33it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [00:46<00:00,  6.54it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.6045751633986928\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [00:19<00:00,  7.00it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
    "Processing college_medicine: 100%|████████████| 173/173 [00:34<00:00,  5.01it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
    "Processing college_biology: 100%|█████████████| 144/144 [00:23<00:00,  6.12it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
    "Overall MMLU Accuracy: 0.5759651307596513\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5759651307596513\n",
    "--------------------------------------------------\n",
    "3-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [02:42<00:00,  1.91it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6516129032258065\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [00:50<00:00,  1.99it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.65\n",
    "Processing virology: 100%|████████████████████| 166/166 [01:23<00:00,  1.99it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.4819277108433735\n",
    "Processing professional_medicine: 100%|███████| 272/272 [02:57<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [02:38<00:00,  1.93it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.5980392156862745\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [01:08<00:00,  1.97it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
    "Processing college_medicine: 100%|████████████| 173/173 [01:38<00:00,  1.76it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5953757225433526\n",
    "Processing college_biology: 100%|█████████████| 144/144 [01:16<00:00,  1.89it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.5833333333333334\n",
    "Overall MMLU Accuracy: 0.5821917808219178\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5821917808219178\n",
    "--------------------------------------------------\n",
    "5-shot\n",
    "Processing high_school_biology: 100%|█████████| 310/310 [03:27<00:00,  1.49it/s]\n",
    "MMLU Task Accuracy (task=high_school_biology): 0.6612903225806451\n",
    "Processing medical_genetics: 100%|████████████| 100/100 [01:04<00:00,  1.54it/s]\n",
    "MMLU Task Accuracy (task=medical_genetics): 0.64\n",
    "Processing virology: 100%|████████████████████| 166/166 [01:48<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=virology): 0.45180722891566266\n",
    "Processing professional_medicine: 100%|███████| 272/272 [03:43<00:00,  1.22it/s]\n",
    "MMLU Task Accuracy (task=professional_medicine): 0.5845588235294118\n",
    "Processing nutrition: 100%|███████████████████| 306/306 [03:22<00:00,  1.51it/s]\n",
    "MMLU Task Accuracy (task=nutrition): 0.5784313725490197\n",
    "Processing anatomy: 100%|█████████████████████| 135/135 [01:28<00:00,  1.53it/s]\n",
    "MMLU Task Accuracy (task=anatomy): 0.43703703703703706\n",
    "Processing college_medicine: 100%|████████████| 173/173 [02:02<00:00,  1.42it/s]\n",
    "MMLU Task Accuracy (task=college_medicine): 0.5895953757225434\n",
    "Processing college_biology: 100%|█████████████| 144/144 [01:36<00:00,  1.49it/s]\n",
    "MMLU Task Accuracy (task=college_biology): 0.6041666666666666\n",
    "Overall MMLU Accuracy: 0.5778331257783312\n",
    "--------------------------------------------------\n",
    "Overall score for BioMistral 7B: 0.5778331257783312 \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models are evaluated using zero-shot prediction.\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Accuracy Metric       | Mistral 7B Instruct | BioMistral 7B | MetaMath 7B | BioLoRA 7B | MetaLoRA 7B | MetaBio 7B | MetaBioLoRA 7B |\n",
      "+=======================+=====================+===============+=============+============+=============+============+================+\n",
      "| Overall Score         | 0.47131             | 0.48019       | -           | -          | -           | 0.49897    | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Overall Score - MATH  | 0.36784             | 0.36213       | -           | -          | -           | \u001b[32m0.36883\u001b[0m    | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Overall Score - BIO   | 0.56791             | 0.59179       | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Math               | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Abstract Algebra      | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Machine Learning      | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Elementary Math       | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Math          | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Formal Logic          | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Stats              | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Clinical KG           | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Medical Genetics      | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Anatomy               | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| Professional Medicine | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Biology       | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| College Medicine      | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n",
      "| HS Biology            | -                   | -             | -           | -          | -           | -          | -              |\n",
      "+-----------------------+---------------------+---------------+-------------+------------+-------------+------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# header = ['Model', 'Overall Score (0-shot)', 'Overall Score (3-shot)', 'Overall Score (5-shot)', 'Overall Score - MATH', 'Overall Score - BIO']\n",
    "# data = [\n",
    "#     # [metamath_7b.get_model_name(), math_benchmark.overall_score],\n",
    "#     ['Mistral 7B Instruct', 0.47131, '-', '-', '-', '-'],\n",
    "#     ['BioMistral 7B', 0.57597, 0.58219, 0.57783, '-', '-'],\n",
    "#     ['-', '-', '-', '-', '-', '-']\n",
    "# ]\n",
    "\n",
    "header = ['Accuracy Metric', 'Mistral 7B Instruct', 'BioMistral 7B', 'MetaMath 7B', 'BioLoRA 7B', 'MetaLoRA 7B', 'MetaBio 7B', 'MetaBioLoRA 7B']\n",
    "data = [\n",
    "    ['Overall Score', 0.47131, 0.48019, '-', '-', '-', 0.49897, '-'],\n",
    "    ['Overall Score - MATH', 0.36784, 0.36213, '-', '-', '-', '\\033[32m0.36883\\033[0m', '-'],\n",
    "    ['Overall Score - BIO', 0.56791, 0.59179, '-', '-', '-', '-', '-'],\n",
    "    ['HS Math', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Abstract Algebra', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Machine Learning', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Elementary Math', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['College Math', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Formal Logic', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['HS Stats', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Clinical KG', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Medical Genetics', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Anatomy', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['Professional Medicine', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['College Biology', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['College Medicine', '-', '-', '-', '-', '-', '-', '-'],\n",
    "    ['HS Biology', '-', '-', '-', '-', '-', '-', '-']\n",
    "]\n",
    "\n",
    "print('All models are evaluated using zero-shot prediction.')\n",
    "tt.print(\n",
    "    data,\n",
    "    header=header,\n",
    "    style=tt.styles.ascii_thin_double,\n",
    "    padding=(0, 1),\n",
    "    #alignment=\"lcr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH\n",
      "0.36784\n",
      "BIO\n",
      "0.56791\n"
     ]
    }
   ],
   "source": [
    "# Mistral 7B Instruct\n",
    "# MATH\n",
    "# MMLU Task Accuracy (task=high_school_mathematics): 0.3037037037037037\n",
    "# MMLU Task Accuracy (task=abstract_algebra): 0.33\n",
    "# MMLU Task Accuracy (task=machine_learning): 0.45535714285714285\n",
    "# MMLU Task Accuracy (task=elementary_mathematics): 0.3412698412698413\n",
    "# MMLU Task Accuracy (task=college_mathematics): 0.38\n",
    "# MMLU Task Accuracy (task=formal_logic): 0.35714285714285715\n",
    "# MMLU Task Accuracy (task=high_school_statistics): 0.4074074074074074\n",
    "# BIO\n",
    "# MMLU Task Accuracy (task=clinical_knowledge): 0.6075471698113207\n",
    "# MMLU Task Accuracy (task=medical_genetics): 0.62\n",
    "# MMLU Task Accuracy (task=anatomy): 0.45925925925925926\n",
    "# MMLU Task Accuracy (task=professional_medicine): 0.5625\n",
    "# MMLU Task Accuracy (task=college_biology): 0.5902777777777778\n",
    "# MMLU Task Accuracy (task=college_medicine): 0.49710982658959535\n",
    "# MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "\n",
    "print('MATH')\n",
    "print(np.round((0.3037037037037037 + 0.33 + 0.45535714285714285 + 0.3412698412698413 + 0.38 + 0.35714285714285715 + 0.4074074074074074) / 7, 5))\n",
    "print('BIO')\n",
    "print(np.round((0.6075471698113207 + 0.62 + 0.45925925925925926 + 0.5625 + 0.5902777777777778 + 0.49710982658959535 + 0.6387096774193548) / 7, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH\n",
      "0.36213\n",
      "BIO\n",
      "0.59179\n"
     ]
    }
   ],
   "source": [
    "# BioMistral 7B\n",
    "# MATH\n",
    "# MMLU Task Accuracy (task=high_school_mathematics): 0.3148148148148148\n",
    "# MMLU Task Accuracy (task=abstract_algebra): 0.3\n",
    "# MMLU Task Accuracy (task=machine_learning): 0.4642857142857143\n",
    "# MMLU Task Accuracy (task=elementary_mathematics): 0.34656084656084657\n",
    "# MMLU Task Accuracy (task=college_mathematics): 0.35\n",
    "# MMLU Task Accuracy (task=formal_logic): 0.3333333333333333\n",
    "# MMLU Task Accuracy (task=high_school_statistics): 0.42592592592592593\n",
    "# BIO\n",
    "# MMLU Task Accuracy (task=clinical_knowledge): 0.6188679245283019\n",
    "# MMLU Task Accuracy (task=medical_genetics): 0.67\n",
    "# MMLU Task Accuracy (task=anatomy): 0.4962962962962963\n",
    "# MMLU Task Accuracy (task=professional_medicine): 0.5514705882352942\n",
    "# MMLU Task Accuracy (task=college_biology): 0.6180555555555556\n",
    "# MMLU Task Accuracy (task=college_medicine): 0.5491329479768786\n",
    "# MMLU Task Accuracy (task=high_school_biology): 0.6387096774193548\n",
    "\n",
    "print('MATH')\n",
    "print(np.round((0.3148148148148148 + 0.3 + 0.4642857142857143 + 0.34656084656084657 + 0.35 + 0.3333333333333333 + 0.42592592592592593) / 7, 5))\n",
    "print('BIO')\n",
    "print(np.round((0.6188679245283019 + 0.67 + 0.4962962962962963 + 0.5514705882352942 + 0.6180555555555556 + 0.5491329479768786 + 0.6387096774193548) / 7, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36883"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MetaBio\n",
    "# MATH\n",
    "np.round((0.307407 + 0.300000 + 0.482143 + 0.349206 + 0.360000 + 0.357143 + 0.425926) / 7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
